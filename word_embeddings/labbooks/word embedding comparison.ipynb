{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding is the collective name for a set of language modeling and feature learning techniques aimed to map a single text-words into a fixed dimension real-valued vector space.  It is also often called distributed word representation.\n",
    "\n",
    "Why do we need to encode words?\n",
    "Many Machine Learning algorithms and almost all Deep Learning Architectures are incapable of processing strings or plain text in their raw form. They require numbers as inputs to perform any sort of job, be it a classification, regression etc.\n",
    "\n",
    "One of the ways to encode words is one-hot encoding. One-hot encoding is a vector representation where all the elements of the vector are 0 except one, which has 1 as its value. The main drawback of this type of representation is a large vector dimension.  Therefore, this approach will be too memory consuming. In addition, we can’t find the similarities between different words in this way as each word is represented differently and there is no way to compare them.\n",
    "\n",
    "There is a better way to encode words- semantic word embedding. Vector representations of words (word embedding) try to capture relationships between words as distance or angle. The simplest methods use word vectors that explicitly represent co-occurrence statistics. Neural network language models propose another way to construct embedding: the word vector is simply the neural network's internal representation of the word.\n",
    "\n",
    "In this lab notebook, we will compare the standard, pretrained word embedding (GLoVe, Word2Vec, fastText, ELMO) for SQuAD data. In particular, we are interested in the word coverage, that is the percentage of the SQuAD words in the word embedding. We will analyse the impact of different sentence normalization methods on the word coverage level. \n",
    "Every word embedding algorithm would be tested on all pretrained models in open access. \n",
    "\n",
    "Every pretrained model would be described by the set of features:\n",
    "- name: a brief description of the source of tokens\n",
    "- number of words in vocabulary: a number of unique tokens in vocabulary\n",
    "- missing words(missing words percent): number(percent) of words from SQuAD vocabulary which are out of pretrained model vocabulary\n",
    "- first loading time: time needed for pretrained model to be loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Testing sentence processing methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step of testing word embbedding methods we should divide SQuAD data into separate words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ForMaxwell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ForMaxwell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ForMaxwell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\ForMaxwell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../source_code/')\n",
    "import utils as utils\n",
    "import nltk\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en')\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import os\n",
    "java_path = \"C:/ProgramData/Oracle/Java/javapath/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load SQuAD training set\n",
    "training_set = pd.read_json(\"../data/squad/train-v2.0.json\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test named entities recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to correctly determine which entities should be lowercased we will use named entities recognition tools. \n",
    "\n",
    "The ways of NE recognition are described here\n",
    "https://towardsdatascience.com/named-entity-recognition-3fad3f53c91e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kanye Omari West (/ˈkɑːnjeɪ/; born June 8, 1977) is an American hip hop recording artist, record producer, rapper, fashion designer, and entrepreneur.Raised in Chicago, West briefly attended art school before becoming known as a producer for Roc-A-Fella Records in the early 2000s, producing hit singles for artists such as Jay-Z and Alicia Keys.The exact nature of relations between Tibet and the Ming dynasty of China (1368–1644) is unclear.Some Mainland Chinese scholars, such as Wang Jiawei and Nyima Gyaincain, assert that the Ming dynasty had unquestioned sovereignty over Tibet, pointing to the Ming court's issuing of various titles to Tibetan leaders, Tibetans' full acceptance of these titles, and a renewal process for successors of these titles that involved traveling to the Ming capital.Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress.\n",
      "total test word count: 139\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "text =  utils.get_squad_sentences(training_set[10:11])[0] + \\\n",
    "        utils.get_squad_sentences(training_set[10:11])[2] + \\\n",
    "        utils.get_squad_sentences(training_set[2:3])[0] + \\\n",
    "        utils.get_squad_sentences(training_set[2:3])[2] + \\\n",
    "        utils.get_squad_sentences(training_set[0:1])[0]\n",
    "print(text)\n",
    "\n",
    "#entities which must be uppercased\n",
    "named_entities_list = ['Kanye', 'Omari', 'West', 'American', 'Chicago', 'Roc-A-Fella','Records','Jay-Z','Alicia', \\\n",
    "                       'Keys', 'Tibet', 'Ming', 'China', 'Mainland' , 'Chinese', 'Wang' , 'Jiawei', 'Nyima', 'Gyaincain', \\\n",
    "                       'Tibetan', 'Tibetans', 'Beyoncé', 'Giselle' , 'Knowles-Carter', 'September'];\n",
    "\n",
    "words_count = len(text.split())\n",
    "print('total test word count: {0}'.format(words_count))\n",
    "\n",
    "NE_df = pd.DataFrame(columns=['name', 'FPR', 'Recall(TPR)', 'FNR', 'Precision'])\n",
    "NE_df_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chicago', 'Kanye', 'Tibet', 'Jiawei', 'Nyima', 'Chinese', 'Wang', 'Ming', 'Omari', 'Tibetans', 'American', 'China', 'Mainland', 'Gyaincain', 'Tibetan', 'West', 'Alicia', 'Giselle']\n",
      "FPR: 0.000000\n",
      "Recall(TPR): 0.720000\n",
      "FNR: 0.280000\n",
      "Precision: 1.000000\n"
     ]
    }
   ],
   "source": [
    "#using nltk ne_chunk\n",
    "from nltk import Tree, pos_tag, ne_chunk\n",
    "tagged_sent = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "named_entities = []\n",
    "\n",
    "from nltk.sem.relextract import NE_CLASSES\n",
    "ace_tags = NE_CLASSES['ace']\n",
    "\n",
    "for node in tagged_sent:\n",
    "     if type(node) == Tree and node.label() in ace_tags:\n",
    "        words, tags = zip(*node.leaves())\n",
    "        named_entities += words\n",
    "\n",
    "named_entities = utils.remove_duplicates(named_entities)\n",
    "print(named_entities)   \n",
    "\n",
    "utils.get_accuracy(named_entities, named_entities_list, words_count, NE_df, NE_df_number, 'nltk')\n",
    "NE_df_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\nltk\\tag\\stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use nltk.tag.corenlp.CoreNLPPOSTagger or nltk.tag.corenlp.CoreNLPNERTagger instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kanye', 'Omari', 'West', 'Chicago,', 'West', 'Roc-A-Fella', 'Records', 'Jay-Z', 'Alicia', 'Tibet', 'China', 'Wang', 'Jiawei']\n",
      "FPR: 0.008772\n",
      "Recall(TPR): 0.480000\n",
      "FNR: 0.520000\n",
      "Precision: 0.923077\n"
     ]
    }
   ],
   "source": [
    "#using Stanford NER\n",
    "ner_directory = 'C:/MRC/squad/stanford-ner-2018-02-27/'\n",
    "\n",
    "stanford_ner_tagger = StanfordNERTagger(\n",
    "    ner_directory + 'classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "    ner_directory + 'stanford-ner-3.9.1.jar'\n",
    ")\n",
    "\n",
    "results = stanford_ner_tagger.tag(text.split())\n",
    "\n",
    "named_entities = []\n",
    "for result in results:\n",
    "    tag_value = result[0]\n",
    "    tag_type = result[1]\n",
    "    if tag_type != 'O':\n",
    "        named_entities.append(tag_value)\n",
    "        \n",
    "print(named_entities)\n",
    "\n",
    "utils.get_accuracy(named_entities, named_entities_list, words_count, NE_df, NE_df_number, 'Stanford NER')\n",
    "NE_df_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kanye', 'Omari', 'West', 'American', 'Chicago', 'Roc', 'Jay-Z', 'Alicia', 'Keys', 'Tibet', 'China', 'Chinese', 'Wang', 'Jiawei', 'Nyima', 'Gyaincain', 'Tibet', 'Ming', 'Tibetan', 'Tibetans', 'Ming', 'Giselle', 'Knowles-Carter', 'American']\n",
      "FPR: 0.008772\n",
      "Recall(TPR): 0.920000\n",
      "FNR: 0.080000\n",
      "Precision: 0.958333\n"
     ]
    }
   ],
   "source": [
    "#using spacy\n",
    "document = spacy_nlp(text)\n",
    "\n",
    "named_entities = []\n",
    "NE_taggs = ['PERSON' , 'FAC', 'ORG', 'NORP', 'GPE', 'LOC', 'PRODUCT', 'LAW', 'LANGUAGE'] \n",
    "for element in document.ents:\n",
    "    if (element.label_ in NE_taggs):\n",
    "        named_entities += ([word.strip(string.punctuation) for word in element.text.split()])\n",
    "    \n",
    "print(named_entities)\n",
    "\n",
    "utils.get_accuracy(named_entities, named_entities_list, words_count, NE_df, NE_df_number, 'spacy')\n",
    "NE_df_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>FPR</th>\n",
       "      <th>Recall(TPR)</th>\n",
       "      <th>FNR</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nltk</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stanford NER</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spacy</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.958333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name       FPR  Recall(TPR)   FNR  Precision\n",
       "0          nltk  0.000000         0.72  0.28   1.000000\n",
       "1  Stanford NER  0.008772         0.48  0.52   0.923077\n",
       "2         spacy  0.008772         0.92  0.08   0.958333"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NE_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy has shown the best result on test sentence: if has highest  recall and rather high precision, lowerst FNR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test splitting text into separate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kanye Omari West (/ˈkɑːnjeɪ/; born June 8, 1977) is an American hip hop recording artist, record producer, rapper, fashion designer, and entrepreneur.Raised in Chicago, West briefly attended art school before becoming known as a producer for Roc-A-Fella Records in the early 2000s, producing hit singles for artists such as Jay-Z and Alicia Keys.The exact nature of relations between Tibet and the Ming dynasty of China (1368–1644) is unclear.Some Mainland Chinese scholars, such as Wang Jiawei and Nyima Gyaincain, assert that the Ming dynasty had unquestioned sovereignty over Tibet, pointing to the Ming court's issuing of various titles to Tibetan leaders, Tibetans' full acceptance of these titles, and a renewal process for successors of these titles that involved traveling to the Ming capital.Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#regex tokenizer\n",
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text blob founded 1 sentences in text\n",
      "nltk tokenizer founded 1 sentences in text\n",
      "spacy tokenizer founded 6 sentences in text\n",
      "regex tokenizer founded 5 sentences in text\n"
     ]
    }
   ],
   "source": [
    "#textBlob\n",
    "sentences = TextBlob(text).sentences\n",
    "print('text blob founded %i sentences in text' % len(sentences))\n",
    "\n",
    "#nltk tokenizer\n",
    "from nltk import tokenize\n",
    "sentences = tokenize.sent_tokenize(text)\n",
    "print('nltk tokenizer founded %i sentences in text' % len(sentences))\n",
    "\n",
    "#spacy \n",
    "tokens = spacy_nlp(text)\n",
    "sentences = []\n",
    "for sent in tokens.sents:\n",
    "    sentences.append(sent.string.strip())\n",
    "    \n",
    "print('spacy tokenizer founded %i sentences in text' % len(sentences))\n",
    "\n",
    "sentences = split_into_sentences(text)\n",
    "print('regex tokenizer founded %i sentences in text' % len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right count of sentences in text is 5, that is why regex tokenizer has shown the best result. We will use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test splitting sentence into sepatare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kanye Omari West (/ˈkɑːnjeɪ/; born June 8, 1977) is an American hip hop recording artist, record producer, rapper, fashion designer, and entrepreneur.', 'Raised in Chicago, West briefly attended art school before becoming known as a producer for Roc-A-Fella Records in the early 2000s, producing hit singles for artists such as Jay-Z and Alicia Keys.', 'The exact nature of relations between Tibet and the Ming dynasty of China (1368–1644) is unclear.', \"Some Mainland Chinese scholars, such as Wang Jiawei and Nyima Gyaincain, assert that the Ming dynasty had unquestioned sovereignty over Tibet, pointing to the Ming court's issuing of various titles to Tibetan leaders, Tibetans' full acceptance of these titles, and a renewal process for successors of these titles that involved traveling to the Ming capital.\", 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk founded 144 words\n"
     ]
    }
   ],
   "source": [
    "#nltk\n",
    "nltk_words = []\n",
    "for i in range(len(sentences)):\n",
    "    words_tmp = [word.strip(string.punctuation) for word in nltk.word_tokenize(sentences[i])]\n",
    "    nltk_words += list(filter(None, words_tmp))\n",
    "print('nltk founded %i words' % len(nltk_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string founded 143 words\n"
     ]
    }
   ],
   "source": [
    "#string\n",
    "string_words = []\n",
    "for i in range(len(sentences)):\n",
    "    string_words += [word.strip(string.punctuation) for word in sentences[i].split()]\n",
    "print('string founded %i words' % len(string_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shlex founded 135 words\n"
     ]
    }
   ],
   "source": [
    "#shlex \n",
    "import shlex\n",
    "\n",
    "shlex_words = []\n",
    "for i in range(len(sentences)):\n",
    "    shlex_words += shlex.split(sentences[i])\n",
    "print('shlex founded %i words' % len(shlex_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sta founded 143 words\n"
     ]
    }
   ],
   "source": [
    "#sta\n",
    "import sta\n",
    "\n",
    "sta_words = []\n",
    "for i in range(len(sentences)):\n",
    "    sta_words += sta(sentences[i])\n",
    "print('sta founded %i words' % len(sta_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textblob founded 144 words\n"
     ]
    }
   ],
   "source": [
    "#textblob\n",
    "textblob_words = []\n",
    "for i in range(len(sentences)):\n",
    "    textblob_words += TextBlob(sentences[i]).words\n",
    "print('textblob founded %i words' % len(textblob_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kanye Omari West (/ˈkɑːnjeɪ/; born June 8, 1977) is an American hip hop recording artist, record producer, rapper, fashion designer, and entrepreneur.', 'Raised in Chicago, West briefly attended art school before becoming known as a producer for Roc-A-Fella Records in the early 2000s, producing hit singles for artists such as Jay-Z and Alicia Keys.', 'The exact nature of relations between Tibet and the Ming dynasty of China (1368–1644) is unclear.', \"Some Mainland Chinese scholars, such as Wang Jiawei and Nyima Gyaincain, assert that the Ming dynasty had unquestioned sovereignty over Tibet, pointing to the Ming court's issuing of various titles to Tibetan leaders, Tibetans' full acceptance of these titles, and a renewal process for successors of these titles that involved traveling to the Ming capital.\", 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress.']\n",
      "both have ['Kanye', 'Omari', 'West']\n",
      "  1st has ['ˈkɑːnjeɪ']\n",
      "  2nd has ['(/ˈkɑːnjeɪ/;']\n",
      "both have ['born', 'June']\n",
      "  1st has ['8', '1977']\n",
      "  2nd has ['8,', '1977)']\n",
      "both have ['is', 'an', 'American', 'hip', 'hop', 'recording']\n",
      "  1st has ['artist']\n",
      "  2nd has ['artist,']\n",
      "both have ['record']\n",
      "  1st has ['producer', 'rapper']\n",
      "  2nd has ['producer,', 'rapper,']\n",
      "both have ['fashion']\n",
      "  1st has ['designer']\n",
      "  2nd has ['designer,']\n",
      "both have ['and']\n",
      "  1st has ['entrepreneur']\n",
      "  2nd has ['entrepreneur.']\n",
      "both have ['Raised', 'in']\n",
      "  1st has ['Chicago']\n",
      "  2nd has ['Chicago,']\n",
      "both have ['West', 'briefly', 'attended', 'art', 'school', 'before', 'becoming', 'known', 'as', 'a', 'producer', 'for', 'Roc-A-Fella', 'Records', 'in', 'the', 'early']\n",
      "  1st has ['2000s']\n",
      "  2nd has ['2000s,']\n",
      "both have ['producing', 'hit', 'singles', 'for', 'artists', 'such', 'as', 'Jay-Z', 'and', 'Alicia']\n",
      "  1st has ['Keys']\n",
      "  2nd has ['Keys.']\n",
      "both have ['The', 'exact', 'nature', 'of', 'relations', 'between', 'Tibet', 'and', 'the', 'Ming', 'dynasty', 'of', 'China']\n",
      "  1st has ['1368–1644']\n",
      "  2nd has ['(1368–1644)']\n",
      "both have ['is']\n",
      "  1st has ['unclear']\n",
      "  2nd has ['unclear.']\n",
      "both have ['Some', 'Mainland', 'Chinese']\n",
      "  1st has ['scholars']\n",
      "  2nd has ['scholars,']\n",
      "both have ['such', 'as', 'Wang', 'Jiawei', 'and', 'Nyima']\n",
      "  1st has ['Gyaincain']\n",
      "  2nd has ['Gyaincain,']\n",
      "both have ['assert', 'that', 'the', 'Ming', 'dynasty', 'had', 'unquestioned', 'sovereignty', 'over']\n",
      "  1st has ['Tibet']\n",
      "  2nd has ['Tibet,']\n",
      "both have ['pointing', 'to', 'the', 'Ming']\n",
      "  1st has ['court', \"'s\"]\n",
      "  2nd has [\"court's\"]\n",
      "both have ['issuing', 'of', 'various', 'titles', 'to', 'Tibetan']\n",
      "  1st has ['leaders', 'Tibetans']\n",
      "  2nd has ['leaders,', \"Tibetans'\"]\n",
      "both have ['full', 'acceptance', 'of', 'these']\n",
      "  1st has ['titles']\n",
      "  2nd has ['titles,']\n",
      "both have ['and', 'a', 'renewal', 'process', 'for', 'successors', 'of', 'these', 'titles', 'that', 'involved', 'traveling', 'to', 'the', 'Ming']\n",
      "  1st has ['capital']\n",
      "  2nd has ['capital.']\n",
      "both have ['Beyoncé', 'Giselle', 'Knowles-Carter']\n",
      "  1st has ['biːˈjɒnseɪ', 'bee-YON-say', 'born']\n",
      "  2nd has ['(/biːˈjɒnseɪ/', 'bee-YON-say)', '(born']\n",
      "both have ['September']\n",
      "  1st has ['4', '1981']\n",
      "  2nd has ['4,', '1981)']\n",
      "both have ['is', 'an', 'American']\n",
      "  1st has ['singer', 'songwriter']\n",
      "  2nd has ['singer,', 'songwriter,']\n",
      "both have ['record', 'producer', 'and']\n",
      "  1st has ['actress']\n",
      "  2nd has ['actress.']\n"
     ]
    }
   ],
   "source": [
    "#compare textblob_words and sta_words\n",
    "print(sentences)\n",
    "from difflib import SequenceMatcher\n",
    "for tag, i, j, k, l in SequenceMatcher(None, textblob_words, sta_words).get_opcodes():\n",
    "    if tag == 'equal': print('both have', textblob_words[i:j])\n",
    "    if tag in ('delete', 'replace'): print('  1st has', textblob_words[i:j])\n",
    "    if tag in ('insert', 'replace'): print('  2nd has', sta_words[k:l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, textblob split better than sta because textblob exclude useless punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kanye Omari West (/ˈkɑːnjeɪ/; born June 8, 1977) is an American hip hop recording artist, record producer, rapper, fashion designer, and entrepreneur.', 'Raised in Chicago, West briefly attended art school before becoming known as a producer for Roc-A-Fella Records in the early 2000s, producing hit singles for artists such as Jay-Z and Alicia Keys.', 'The exact nature of relations between Tibet and the Ming dynasty of China (1368–1644) is unclear.', \"Some Mainland Chinese scholars, such as Wang Jiawei and Nyima Gyaincain, assert that the Ming dynasty had unquestioned sovereignty over Tibet, pointing to the Ming court's issuing of various titles to Tibetan leaders, Tibetans' full acceptance of these titles, and a renewal process for successors of these titles that involved traveling to the Ming capital.\", 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress.']\n",
      "both have ['Kanye', 'Omari', 'West']\n",
      "  1st has ['ˈkɑːnjeɪ']\n",
      "  2nd has ['(/ˈkɑːnjeɪ/;']\n",
      "both have ['born', 'June']\n",
      "  1st has ['8', '1977']\n",
      "  2nd has ['8,', '1977)']\n",
      "both have ['is', 'an', 'American', 'hip', 'hop', 'recording']\n",
      "  1st has ['artist']\n",
      "  2nd has ['artist,']\n",
      "both have ['record']\n",
      "  1st has ['producer', 'rapper']\n",
      "  2nd has ['producer,', 'rapper,']\n",
      "both have ['fashion']\n",
      "  1st has ['designer']\n",
      "  2nd has ['designer,']\n",
      "both have ['and']\n",
      "  1st has ['entrepreneur']\n",
      "  2nd has ['entrepreneur.']\n",
      "both have ['Raised', 'in']\n",
      "  1st has ['Chicago']\n",
      "  2nd has ['Chicago,']\n",
      "both have ['West', 'briefly', 'attended', 'art', 'school', 'before', 'becoming', 'known', 'as', 'a', 'producer', 'for', 'Roc-A-Fella', 'Records', 'in', 'the', 'early']\n",
      "  1st has ['2000s']\n",
      "  2nd has ['2000s,']\n",
      "both have ['producing', 'hit', 'singles', 'for', 'artists', 'such', 'as', 'Jay-Z', 'and', 'Alicia']\n",
      "  1st has ['Keys']\n",
      "  2nd has ['Keys.']\n",
      "both have ['The', 'exact', 'nature', 'of', 'relations', 'between', 'Tibet', 'and', 'the', 'Ming', 'dynasty', 'of', 'China']\n",
      "  1st has ['1368–1644']\n",
      "  2nd has ['(1368–1644)']\n",
      "both have ['is']\n",
      "  1st has ['unclear']\n",
      "  2nd has ['unclear.']\n",
      "both have ['Some', 'Mainland', 'Chinese']\n",
      "  1st has ['scholars']\n",
      "  2nd has ['scholars,']\n",
      "both have ['such', 'as', 'Wang', 'Jiawei', 'and', 'Nyima']\n",
      "  1st has ['Gyaincain']\n",
      "  2nd has ['Gyaincain,']\n",
      "both have ['assert', 'that', 'the', 'Ming', 'dynasty', 'had', 'unquestioned', 'sovereignty', 'over']\n",
      "  1st has ['Tibet']\n",
      "  2nd has ['Tibet,']\n",
      "both have ['pointing', 'to', 'the', 'Ming']\n",
      "  1st has ['court', \"'s\", 'issuing', 'of', 'various', 'titles', 'to', 'Tibetan', 'leaders', 'Tibetans']\n",
      "  2nd has ['courts issuing of various titles to Tibetan leaders, Tibetans']\n",
      "both have ['full', 'acceptance', 'of', 'these']\n",
      "  1st has ['titles']\n",
      "  2nd has ['titles,']\n",
      "both have ['and', 'a', 'renewal', 'process', 'for', 'successors', 'of', 'these', 'titles', 'that', 'involved', 'traveling', 'to', 'the', 'Ming']\n",
      "  1st has ['capital']\n",
      "  2nd has ['capital.']\n",
      "both have ['Beyoncé', 'Giselle', 'Knowles-Carter']\n",
      "  1st has ['biːˈjɒnseɪ', 'bee-YON-say', 'born']\n",
      "  2nd has ['(/biːˈjɒnseɪ/', 'bee-YON-say)', '(born']\n",
      "both have ['September']\n",
      "  1st has ['4', '1981']\n",
      "  2nd has ['4,', '1981)']\n",
      "both have ['is', 'an', 'American']\n",
      "  1st has ['singer', 'songwriter']\n",
      "  2nd has ['singer,', 'songwriter,']\n",
      "both have ['record', 'producer', 'and']\n",
      "  1st has ['actress']\n",
      "  2nd has ['actress.']\n"
     ]
    }
   ],
   "source": [
    "#compare textblob_words and shlex_words\n",
    "print(sentences)\n",
    "from difflib import SequenceMatcher\n",
    "for tag, i, j, k, l in SequenceMatcher(None, textblob_words, shlex_words).get_opcodes():\n",
    "    if tag == 'equal': print('both have', textblob_words[i:j])\n",
    "    if tag in ('delete', 'replace'): print('  1st has', textblob_words[i:j])\n",
    "    if tag in ('insert', 'replace'): print('  2nd has', shlex_words[k:l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, textblob split better than sta because textblob exclude useless punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kanye Omari West (/ˈkɑːnjeɪ/; born June 8, 1977) is an American hip hop recording artist, record producer, rapper, fashion designer, and entrepreneur.', 'Raised in Chicago, West briefly attended art school before becoming known as a producer for Roc-A-Fella Records in the early 2000s, producing hit singles for artists such as Jay-Z and Alicia Keys.', 'The exact nature of relations between Tibet and the Ming dynasty of China (1368–1644) is unclear.', \"Some Mainland Chinese scholars, such as Wang Jiawei and Nyima Gyaincain, assert that the Ming dynasty had unquestioned sovereignty over Tibet, pointing to the Ming court's issuing of various titles to Tibetan leaders, Tibetans' full acceptance of these titles, and a renewal process for successors of these titles that involved traveling to the Ming capital.\", 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress.']\n",
      "both have ['Kanye', 'Omari', 'West', 'ˈkɑːnjeɪ', 'born', 'June', '8', '1977', 'is', 'an', 'American', 'hip', 'hop', 'recording', 'artist', 'record', 'producer', 'rapper', 'fashion', 'designer', 'and', 'entrepreneur', 'Raised', 'in', 'Chicago', 'West', 'briefly', 'attended', 'art', 'school', 'before', 'becoming', 'known', 'as', 'a', 'producer', 'for', 'Roc-A-Fella', 'Records', 'in', 'the', 'early', '2000s', 'producing', 'hit', 'singles', 'for', 'artists', 'such', 'as', 'Jay-Z', 'and', 'Alicia', 'Keys', 'The', 'exact', 'nature', 'of', 'relations', 'between', 'Tibet', 'and', 'the', 'Ming', 'dynasty', 'of', 'China', '1368–1644', 'is', 'unclear', 'Some', 'Mainland', 'Chinese', 'scholars', 'such', 'as', 'Wang', 'Jiawei', 'and', 'Nyima', 'Gyaincain', 'assert', 'that', 'the', 'Ming', 'dynasty', 'had', 'unquestioned', 'sovereignty', 'over', 'Tibet', 'pointing', 'to', 'the', 'Ming']\n",
      "  1st has ['court', \"'s\"]\n",
      "  2nd has [\"court's\"]\n",
      "both have ['issuing', 'of', 'various', 'titles', 'to', 'Tibetan', 'leaders', 'Tibetans', 'full', 'acceptance', 'of', 'these', 'titles', 'and', 'a', 'renewal', 'process', 'for', 'successors', 'of', 'these', 'titles', 'that', 'involved', 'traveling', 'to', 'the', 'Ming', 'capital', 'Beyoncé', 'Giselle', 'Knowles-Carter', 'biːˈjɒnseɪ', 'bee-YON-say', 'born', 'September', '4', '1981', 'is', 'an', 'American', 'singer', 'songwriter', 'record', 'producer', 'and', 'actress']\n"
     ]
    }
   ],
   "source": [
    "#compare textblob_words and string_words\n",
    "print(sentences)\n",
    "from difflib import SequenceMatcher\n",
    "for tag, i, j, k, l in SequenceMatcher(None, textblob_words, string_words).get_opcodes():\n",
    "    if tag == 'equal': print('both have', textblob_words[i:j])\n",
    "    if tag in ('delete', 'replace'): print('  1st has', textblob_words[i:j])\n",
    "    if tag in ('insert', 'replace'): print('  2nd has', string_words[k:l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, string split better than textblob because the word 'court's' shouldn't be separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kanye Omari West (/ˈkɑːnjeɪ/; born June 8, 1977) is an American hip hop recording artist, record producer, rapper, fashion designer, and entrepreneur.', 'Raised in Chicago, West briefly attended art school before becoming known as a producer for Roc-A-Fella Records in the early 2000s, producing hit singles for artists such as Jay-Z and Alicia Keys.', 'The exact nature of relations between Tibet and the Ming dynasty of China (1368–1644) is unclear.', \"Some Mainland Chinese scholars, such as Wang Jiawei and Nyima Gyaincain, assert that the Ming dynasty had unquestioned sovereignty over Tibet, pointing to the Ming court's issuing of various titles to Tibetan leaders, Tibetans' full acceptance of these titles, and a renewal process for successors of these titles that involved traveling to the Ming capital.\", 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress.']\n",
      "both have ['Kanye', 'Omari', 'West', 'ˈkɑːnjeɪ', 'born', 'June', '8', '1977', 'is', 'an', 'American', 'hip', 'hop', 'recording', 'artist', 'record', 'producer', 'rapper', 'fashion', 'designer', 'and', 'entrepreneur', 'Raised', 'in', 'Chicago', 'West', 'briefly', 'attended', 'art', 'school', 'before', 'becoming', 'known', 'as', 'a', 'producer', 'for', 'Roc-A-Fella', 'Records', 'in', 'the', 'early', '2000s', 'producing', 'hit', 'singles', 'for', 'artists', 'such', 'as', 'Jay-Z', 'and', 'Alicia', 'Keys', 'The', 'exact', 'nature', 'of', 'relations', 'between', 'Tibet', 'and', 'the', 'Ming', 'dynasty', 'of', 'China', '1368–1644', 'is', 'unclear', 'Some', 'Mainland', 'Chinese', 'scholars', 'such', 'as', 'Wang', 'Jiawei', 'and', 'Nyima', 'Gyaincain', 'assert', 'that', 'the', 'Ming', 'dynasty', 'had', 'unquestioned', 'sovereignty', 'over', 'Tibet', 'pointing', 'to', 'the', 'Ming']\n",
      "  1st has [\"court's\"]\n",
      "  2nd has ['court', 's']\n",
      "both have ['issuing', 'of', 'various', 'titles', 'to', 'Tibetan', 'leaders', 'Tibetans', 'full', 'acceptance', 'of', 'these', 'titles', 'and', 'a', 'renewal', 'process', 'for', 'successors', 'of', 'these', 'titles', 'that', 'involved', 'traveling', 'to', 'the', 'Ming', 'capital', 'Beyoncé', 'Giselle', 'Knowles-Carter', 'biːˈjɒnseɪ', 'bee-YON-say', 'born', 'September', '4', '1981', 'is', 'an', 'American', 'singer', 'songwriter', 'record', 'producer', 'and', 'actress']\n"
     ]
    }
   ],
   "source": [
    "#compare string_words and nltk_words\n",
    "print(sentences)\n",
    "from difflib import SequenceMatcher\n",
    "for tag, i, j, k, l in SequenceMatcher(None, string_words, nltk_words).get_opcodes():\n",
    "    if tag == 'equal': print('both have', string_words[i:j])\n",
    "    if tag in ('delete', 'replace'): print('  1st has', string_words[i:j])\n",
    "    if tag in ('insert', 'replace'): print('  2nd has', nltk_words[k:l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the difference between nltk and string splitter is the way of splitting the word 'court's'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to split a sentence into separate words is using the nltk. If the ''s' is separated, a neural network trained on this dictionary can learn its semantic, meaning that it indicates possession. If the 'courd's' stays together, then for every single noun in the English dictionary, there will be another token for the possession, like apple -> apple's. Moreover, if there are no words like apple's in the training data, a neural network trained on this dictionary will never generalize to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking nltk string splitting features, improving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text for testing nltk string split results:  ['Beyoncé\\'s interest in music and performing continued after winning a school talent show at age seven, singing John Lennon\\'s \"Imagine\" to beat 15/16-year-olds.', 'Beyoncé\\'s interest in music and performing continued after winning a school talent show at age seven, singing John Lennon\\'s \"Imagine\" to beat 15/16-year-olds.', \"Later iPods switched fonts again to Podium Sans—a font similar to Apple's corporate font, Myriad.\", 'New York—often called New York City or the City of New York to distinguish it from the State of New York, of which it is a part—is the most populous city in the United States and the center of the New York metropolitan area, the premier gateway for legal immigration to the United States and one of the most populous urban agglomerations in the world.', 'Three of his albums rank on Rolling Stone\\'s 2012 \"500 Greatest Albums of All Time\" list; two of his albums feature at first and eighth, respectively, in Pitchfork Media\\'s The 100 Best Albums of 2010–2014.', 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress.', \"Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child.\"]\n",
      "nltk founded 221 words:  ['Beyoncé', 's', 'interest', 'in', 'music', 'and', 'performing', 'continued', 'after', 'winning', 'a', 'school', 'talent', 'show', 'at', 'age', 'seven', 'singing', 'John', 'Lennon', 's', 'Imagine', 'to', 'beat', '15/16-year-olds', 'Beyoncé', 's', 'interest', 'in', 'music', 'and', 'performing', 'continued', 'after', 'winning', 'a', 'school', 'talent', 'show', 'at', 'age', 'seven', 'singing', 'John', 'Lennon', 's', 'Imagine', 'to', 'beat', '15/16-year-olds', 'Later', 'iPods', 'switched', 'fonts', 'again', 'to', 'Podium', 'Sans—a', 'font', 'similar', 'to', 'Apple', 's', 'corporate', 'font', 'Myriad', 'New', 'York—often', 'called', 'New', 'York', 'City', 'or', 'the', 'City', 'of', 'New', 'York', 'to', 'distinguish', 'it', 'from', 'the', 'State', 'of', 'New', 'York', 'of', 'which', 'it', 'is', 'a', 'part—is', 'the', 'most', 'populous', 'city', 'in', 'the', 'United', 'States', 'and', 'the', 'center', 'of', 'the', 'New', 'York', 'metropolitan', 'area', 'the', 'premier', 'gateway', 'for', 'legal', 'immigration', 'to', 'the', 'United', 'States', 'and', 'one', 'of', 'the', 'most', 'populous', 'urban', 'agglomerations', 'in', 'the', 'world', 'Three', 'of', 'his', 'albums', 'rank', 'on', 'Rolling', 'Stone', 's', '2012', '500', 'Greatest', 'Albums', 'of', 'All', 'Time', 'list', 'two', 'of', 'his', 'albums', 'feature', 'at', 'first', 'and', 'eighth', 'respectively', 'in', 'Pitchfork', 'Media', 's', 'The', '100', 'Best', 'Albums', 'of', '2010–2014', 'Beyoncé', 'Giselle', 'Knowles-Carter', 'biːˈjɒnseɪ', 'bee-YON-say', 'born', 'September', '4', '1981', 'is', 'an', 'American', 'singer', 'songwriter', 'record', 'producer', 'and', 'actress', 'Born', 'and', 'raised', 'in', 'Houston', 'Texas', 'she', 'performed', 'in', 'various', 'singing', 'and', 'dancing', 'competitions', 'as', 'a', 'child', 'and', 'rose', 'to', 'fame', 'in', 'the', 'late', '1990s', 'as', 'lead', 'singer', 'of', 'R', 'B', 'girl-group', 'Destiny', 's', 'Child']\n"
     ]
    }
   ],
   "source": [
    "#new test text\n",
    "text =  utils.get_squad_sentences(training_set[0:1])[28:29] + \\\n",
    "        utils.get_squad_sentences(training_set[0:1])[28:29] + \\\n",
    "        utils.get_squad_sentences(training_set[3:4])[28:29] + \\\n",
    "        utils.get_squad_sentences(training_set[7:8])[0:1] + \\\n",
    "        utils.get_squad_sentences(training_set[10:11])[10:11] + \\\n",
    "        utils.get_squad_sentences(training_set[0:1])[0:2]\n",
    "\n",
    "print('Text for testing nltk string split results: ', text)   \n",
    "\n",
    "nltk_words = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    words_tmp = [word.strip(string.punctuation) for word in nltk.word_tokenize(text[i])]\n",
    "    nltk_words += list(filter(None, words_tmp))\n",
    "print('nltk founded %i words: ' % len(nltk_words), nltk_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion. Some words, which contains non-letter tokens, were not splitted: '15/16-year-olds', 'Sans—a', 'York—often', 'part—is', '2010–2014'. All of these words would not be covered by word embedding algorithms. In order to avoid this incorrect behavior let's try using regex to split words in conjunction with nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk+regex founded 227 words:  ['Beyoncé', 's', 'interest', 'in', 'music', 'and', 'performing', 'continued', 'after', 'winning', 'a', 'school', 'talent', 'show', 'at', 'age', 'seven', 'singing', 'John', 'Lennon', 's', 'Imagine', 'to', 'beat', '15', '16-year-olds', 'Beyoncé', 's', 'interest', 'in', 'music', 'and', 'performing', 'continued', 'after', 'winning', 'a', 'school', 'talent', 'show', 'at', 'age', 'seven', 'singing', 'John', 'Lennon', 's', 'Imagine', 'to', 'beat', '15', '16-year-olds', 'Later', 'iPods', 'switched', 'fonts', 'again', 'to', 'Podium', 'Sans', 'a', 'font', 'similar', 'to', 'Apple', 's', 'corporate', 'font', 'Myriad', 'New', 'York', 'often', 'called', 'New', 'York', 'City', 'or', 'the', 'City', 'of', 'New', 'York', 'to', 'distinguish', 'it', 'from', 'the', 'State', 'of', 'New', 'York', 'of', 'which', 'it', 'is', 'a', 'part', 'is', 'the', 'most', 'populous', 'city', 'in', 'the', 'United', 'States', 'and', 'the', 'center', 'of', 'the', 'New', 'York', 'metropolitan', 'area', 'the', 'premier', 'gateway', 'for', 'legal', 'immigration', 'to', 'the', 'United', 'States', 'and', 'one', 'of', 'the', 'most', 'populous', 'urban', 'agglomerations', 'in', 'the', 'world', 'Three', 'of', 'his', 'albums', 'rank', 'on', 'Rolling', 'Stone', 's', '2012', '500', 'Greatest', 'Albums', 'of', 'All', 'Time', 'list', 'two', 'of', 'his', 'albums', 'feature', 'at', 'first', 'and', 'eighth', 'respectively', 'in', 'Pitchfork', 'Media', 's', 'The', '100', 'Best', 'Albums', 'of', '2010', '2014', 'Beyoncé', 'Giselle', 'Knowles-Carter', 'biːˈjɒnseɪ', 'bee-YON-say', 'born', 'September', '4', '1981', 'is', 'an', 'American', 'singer', 'songwriter', 'record', 'producer', 'and', 'actress', 'Born', 'and', 'raised', 'in', 'Houston', 'Texas', 'she', 'performed', 'in', 'various', 'singing', 'and', 'dancing', 'competitions', 'as', 'a', 'child', 'and', 'rose', 'to', 'fame', 'in', 'the', 'late', '1990s', 'as', 'lead', 'singer', 'of', 'R', 'B', 'girl-group', 'Destiny', 's', 'Child']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "nltk_words_re = []\n",
    "non_letter_regex = '[\\(\\)\\[\\]:;–—/\\\\\\\\]+'\n",
    "for i in range(len(text)):\n",
    "    words_tmp = [word.strip(string.punctuation) for word in nltk.word_tokenize(text[i])]\n",
    "    words_tmp_regex = []\n",
    "    for j in range(len(words_tmp)):\n",
    "        words_tmp_regex += re.split(non_letter_regex, words_tmp[j])\n",
    "    nltk_words_re += list(filter(None, words_tmp_regex))\n",
    "print('nltk+regex founded %i words: ' % len(nltk_words_re), nltk_words_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both have ['Beyoncé', 's', 'interest', 'in', 'music', 'and', 'performing', 'continued', 'after', 'winning', 'a', 'school', 'talent', 'show', 'at', 'age', 'seven', 'singing', 'John', 'Lennon', 's', 'Imagine', 'to', 'beat']\n",
      "  1st has ['15/16-year-olds']\n",
      "  2nd has ['15', '16-year-olds']\n",
      "both have ['Beyoncé', 's', 'interest', 'in', 'music', 'and', 'performing', 'continued', 'after', 'winning', 'a', 'school', 'talent', 'show', 'at', 'age', 'seven', 'singing', 'John', 'Lennon', 's', 'Imagine', 'to', 'beat']\n",
      "  1st has ['15/16-year-olds']\n",
      "  2nd has ['15', '16-year-olds']\n",
      "both have ['Later', 'iPods', 'switched', 'fonts', 'again', 'to', 'Podium']\n",
      "  1st has ['Sans—a']\n",
      "  2nd has ['Sans', 'a']\n",
      "both have ['font', 'similar', 'to', 'Apple', 's', 'corporate', 'font', 'Myriad', 'New']\n",
      "  1st has ['York—often']\n",
      "  2nd has ['York', 'often']\n",
      "both have ['called', 'New', 'York', 'City', 'or', 'the', 'City', 'of', 'New', 'York', 'to', 'distinguish', 'it', 'from', 'the', 'State', 'of', 'New', 'York', 'of', 'which', 'it', 'is', 'a']\n",
      "  1st has ['part—is']\n",
      "  2nd has ['part', 'is']\n",
      "both have ['the', 'most', 'populous', 'city', 'in', 'the', 'United', 'States', 'and', 'the', 'center', 'of', 'the', 'New', 'York', 'metropolitan', 'area', 'the', 'premier', 'gateway', 'for', 'legal', 'immigration', 'to', 'the', 'United', 'States', 'and', 'one', 'of', 'the', 'most', 'populous', 'urban', 'agglomerations', 'in', 'the', 'world', 'Three', 'of', 'his', 'albums', 'rank', 'on', 'Rolling', 'Stone', 's', '2012', '500', 'Greatest', 'Albums', 'of', 'All', 'Time', 'list', 'two', 'of', 'his', 'albums', 'feature', 'at', 'first', 'and', 'eighth', 'respectively', 'in', 'Pitchfork', 'Media', 's', 'The', '100', 'Best', 'Albums', 'of']\n",
      "  1st has ['2010–2014']\n",
      "  2nd has ['2010', '2014']\n",
      "both have ['Beyoncé', 'Giselle', 'Knowles-Carter', 'biːˈjɒnseɪ', 'bee-YON-say', 'born', 'September', '4', '1981', 'is', 'an', 'American', 'singer', 'songwriter', 'record', 'producer', 'and', 'actress', 'Born', 'and', 'raised', 'in', 'Houston', 'Texas', 'she', 'performed', 'in', 'various', 'singing', 'and', 'dancing', 'competitions', 'as', 'a', 'child', 'and', 'rose', 'to', 'fame', 'in', 'the', 'late', '1990s', 'as', 'lead', 'singer', 'of', 'R', 'B', 'girl-group', 'Destiny', 's', 'Child']\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "for tag, i, j, k, l in SequenceMatcher(None, nltk_words, nltk_words_re).get_opcodes():\n",
    "    if tag == 'equal': print('both have', nltk_words[i:j])\n",
    "    if tag in ('delete', 'replace'): print('  1st has', nltk_words[i:j])\n",
    "    if tag in ('insert', 'replace'): print('  2nd has', nltk_words_re[k:l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion. For sentence splitting, we will use nltk + regex. This approach allows dividing sentence even if it has uncommon separators, like [] () / and o on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split SQuAD data to separate words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step we will split text to sepatare sentences with regex tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences in SQuAD context and question sections is : 185169\n"
     ]
    }
   ],
   "source": [
    "sentences = utils.get_squad_sentences(training_set)\n",
    "print(\"The number of sentences in SQuAD context and question sections is : {}\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 185169/185169 [1:28:51<00:00, 34.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Time: 5335.73 sec.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "squad_words_and_freqs = utils.get_words_and_freqs(sentences)\n",
    "print(\"Done. Time: {} sec.\".format(round(time.time()-start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary contains 109584 unique tokens\n"
     ]
    }
   ],
   "source": [
    "print(\"The vocabulary contains {} unique tokens\".format(len(squad_words_and_freqs.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary contains 95465 unique uncased words\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_uncased = utils.get_uncased_word_map(squad_words_and_freqs)\n",
    "print(\"The vocabulary contains {} unique uncased words\".format(len(squad_words_and_freqs_uncased.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 separate words from SQuAD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Giselle',\n",
       " 'Knowles-Carter',\n",
       " 'American',\n",
       " 'beyoncé',\n",
       " 'biːˈjɒnseɪ',\n",
       " 'bee-yon-say',\n",
       " 'born',\n",
       " 'september',\n",
       " '4',\n",
       " '1981',\n",
       " 'is',\n",
       " 'an',\n",
       " 'singer',\n",
       " 'songwriter',\n",
       " 'record',\n",
       " 'producer',\n",
       " 'and',\n",
       " 'actress',\n",
       " 'Houston',\n",
       " 'Texas',\n",
       " 'R',\n",
       " '',\n",
       " 'B',\n",
       " 'Child',\n",
       " 'raised',\n",
       " 'in',\n",
       " 'she',\n",
       " 'performed',\n",
       " 'various',\n",
       " 'singing',\n",
       " 'dancing',\n",
       " 'competitions',\n",
       " 'as',\n",
       " 'a',\n",
       " 'child',\n",
       " 'rose',\n",
       " 'to',\n",
       " 'fame',\n",
       " 'the',\n",
       " 'late',\n",
       " '1990s',\n",
       " 'lead',\n",
       " 'of',\n",
       " 'girl-group',\n",
       " 'destiny',\n",
       " 's',\n",
       " 'Mathew',\n",
       " 'Knowles',\n",
       " 'managed',\n",
       " 'by',\n",
       " 'her',\n",
       " 'father',\n",
       " 'group',\n",
       " 'became',\n",
       " 'one',\n",
       " 'world',\n",
       " 'best-selling',\n",
       " 'girl',\n",
       " 'groups',\n",
       " 'all',\n",
       " 'time',\n",
       " 'Grammy',\n",
       " 'Awards',\n",
       " 'their',\n",
       " 'hiatus',\n",
       " 'saw',\n",
       " 'release',\n",
       " 'debut',\n",
       " 'album',\n",
       " 'dangerously',\n",
       " 'love',\n",
       " '2003',\n",
       " 'which',\n",
       " 'established',\n",
       " 'solo',\n",
       " 'artist',\n",
       " 'worldwide',\n",
       " 'earned',\n",
       " 'five',\n",
       " 'featured',\n",
       " 'billboard',\n",
       " 'hot',\n",
       " '100',\n",
       " 'number-one',\n",
       " 'singles',\n",
       " 'crazy',\n",
       " 'baby',\n",
       " 'boy',\n",
       " \"B'Day\",\n",
       " 'following',\n",
       " 'disbandment',\n",
       " 'june',\n",
       " '2005',\n",
       " 'released',\n",
       " 'second',\n",
       " '2006',\n",
       " 'contained',\n",
       " 'hits',\n",
       " 'déjà',\n",
       " 'vu',\n",
       " 'irreplaceable',\n",
       " 'beautiful',\n",
       " 'liar',\n",
       " 'Golden',\n",
       " 'Globe',\n",
       " 'Dreamgirls',\n",
       " 'also',\n",
       " 'ventured',\n",
       " 'into',\n",
       " 'acting',\n",
       " 'with',\n",
       " 'nominated',\n",
       " 'performance',\n",
       " 'starring',\n",
       " 'roles',\n",
       " 'pink',\n",
       " 'panther',\n",
       " 'obsessed',\n",
       " '2009',\n",
       " 'Jay',\n",
       " 'Z',\n",
       " 'Etta',\n",
       " 'James',\n",
       " 'Cadillac',\n",
       " 'Records',\n",
       " 'marriage',\n",
       " 'rapper',\n",
       " 'portrayal',\n",
       " '2008',\n",
       " 'influenced',\n",
       " 'third',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Sasha',\n",
       " 'Fierce',\n",
       " 'Song',\n",
       " 'birth',\n",
       " 'alter-ego',\n",
       " 'record-setting',\n",
       " 'six',\n",
       " '2010',\n",
       " 'including',\n",
       " 'year',\n",
       " 'for',\n",
       " 'single',\n",
       " 'ladies',\n",
       " 'put',\n",
       " 'ring',\n",
       " 'on',\n",
       " 'it',\n",
       " 'took',\n",
       " 'from',\n",
       " 'music',\n",
       " 'over',\n",
       " 'management',\n",
       " 'career',\n",
       " 'fourth',\n",
       " '2011',\n",
       " 'was',\n",
       " 'subsequently',\n",
       " 'mellower',\n",
       " 'tone',\n",
       " 'exploring',\n",
       " '1970s',\n",
       " 'funk',\n",
       " '1980s',\n",
       " 'pop',\n",
       " 'soul',\n",
       " 'critically',\n",
       " 'acclaimed',\n",
       " 'fifth',\n",
       " 'studio',\n",
       " '2013',\n",
       " 'distinguished',\n",
       " 'previous',\n",
       " 'releases',\n",
       " 'its',\n",
       " 'experimental',\n",
       " 'production',\n",
       " 'exploration',\n",
       " 'darker',\n",
       " 'themes',\n",
       " 'self-described',\n",
       " 'modern-day',\n",
       " 'feminist',\n",
       " 'creates',\n",
       " 'songs',\n",
       " 'that',\n",
       " 'are',\n",
       " 'often',\n",
       " 'characterized',\n",
       " 'relationships',\n",
       " 'monogamy',\n",
       " 'well',\n",
       " 'female',\n",
       " 'sexuality',\n",
       " 'empowerment',\n",
       " 'stage',\n",
       " 'dynamic',\n",
       " 'highly']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('200 separate words from SQuAD')\n",
    "utils.get_word_list(squad_words_and_freqs)[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#words with only english words\n",
    "squad_words_and_freqs_eng = utils.remove_non_latin_words_from_map(squad_words_and_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 separate words with latin letters from SQuAD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Giselle',\n",
       " 'Knowles-Carter',\n",
       " 'American',\n",
       " 'bee-yon-say',\n",
       " 'born',\n",
       " 'september',\n",
       " '4',\n",
       " '1981',\n",
       " 'is',\n",
       " 'an',\n",
       " 'singer',\n",
       " 'songwriter',\n",
       " 'record',\n",
       " 'producer',\n",
       " 'and',\n",
       " 'actress',\n",
       " 'Houston',\n",
       " 'Texas',\n",
       " 'R',\n",
       " '',\n",
       " 'B',\n",
       " 'Child',\n",
       " 'raised',\n",
       " 'in',\n",
       " 'she',\n",
       " 'performed',\n",
       " 'various',\n",
       " 'singing',\n",
       " 'dancing',\n",
       " 'competitions',\n",
       " 'as',\n",
       " 'a',\n",
       " 'child',\n",
       " 'rose',\n",
       " 'to',\n",
       " 'fame',\n",
       " 'the',\n",
       " 'late',\n",
       " '1990s',\n",
       " 'lead',\n",
       " 'of',\n",
       " 'girl-group',\n",
       " 'destiny',\n",
       " 's',\n",
       " 'Mathew',\n",
       " 'Knowles',\n",
       " 'managed',\n",
       " 'by',\n",
       " 'her',\n",
       " 'father',\n",
       " 'group',\n",
       " 'became',\n",
       " 'one',\n",
       " 'world',\n",
       " 'best-selling',\n",
       " 'girl',\n",
       " 'groups',\n",
       " 'all',\n",
       " 'time',\n",
       " 'Grammy',\n",
       " 'Awards',\n",
       " 'their',\n",
       " 'hiatus',\n",
       " 'saw',\n",
       " 'release',\n",
       " 'debut',\n",
       " 'album',\n",
       " 'dangerously',\n",
       " 'love',\n",
       " '2003',\n",
       " 'which',\n",
       " 'established',\n",
       " 'solo',\n",
       " 'artist',\n",
       " 'worldwide',\n",
       " 'earned',\n",
       " 'five',\n",
       " 'featured',\n",
       " 'billboard',\n",
       " 'hot',\n",
       " '100',\n",
       " 'number-one',\n",
       " 'singles',\n",
       " 'crazy',\n",
       " 'baby',\n",
       " 'boy',\n",
       " \"B'Day\",\n",
       " 'following',\n",
       " 'disbandment',\n",
       " 'june',\n",
       " '2005',\n",
       " 'released',\n",
       " 'second',\n",
       " '2006',\n",
       " 'contained',\n",
       " 'hits',\n",
       " 'vu',\n",
       " 'irreplaceable',\n",
       " 'beautiful',\n",
       " 'liar',\n",
       " 'Golden',\n",
       " 'Globe',\n",
       " 'Dreamgirls',\n",
       " 'also',\n",
       " 'ventured',\n",
       " 'into',\n",
       " 'acting',\n",
       " 'with',\n",
       " 'nominated',\n",
       " 'performance',\n",
       " 'starring',\n",
       " 'roles',\n",
       " 'pink',\n",
       " 'panther',\n",
       " 'obsessed',\n",
       " '2009',\n",
       " 'Jay',\n",
       " 'Z',\n",
       " 'Etta',\n",
       " 'James',\n",
       " 'Cadillac',\n",
       " 'Records',\n",
       " 'marriage',\n",
       " 'rapper',\n",
       " 'portrayal',\n",
       " '2008',\n",
       " 'influenced',\n",
       " 'third',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Sasha',\n",
       " 'Fierce',\n",
       " 'Song',\n",
       " 'birth',\n",
       " 'alter-ego',\n",
       " 'record-setting',\n",
       " 'six',\n",
       " '2010',\n",
       " 'including',\n",
       " 'year',\n",
       " 'for',\n",
       " 'single',\n",
       " 'ladies',\n",
       " 'put',\n",
       " 'ring',\n",
       " 'on',\n",
       " 'it',\n",
       " 'took',\n",
       " 'from',\n",
       " 'music',\n",
       " 'over',\n",
       " 'management',\n",
       " 'career',\n",
       " 'fourth',\n",
       " '2011',\n",
       " 'was',\n",
       " 'subsequently',\n",
       " 'mellower',\n",
       " 'tone',\n",
       " 'exploring',\n",
       " '1970s',\n",
       " 'funk',\n",
       " '1980s',\n",
       " 'pop',\n",
       " 'soul',\n",
       " 'critically',\n",
       " 'acclaimed',\n",
       " 'fifth',\n",
       " 'studio',\n",
       " '2013',\n",
       " 'distinguished',\n",
       " 'previous',\n",
       " 'releases',\n",
       " 'its',\n",
       " 'experimental',\n",
       " 'production',\n",
       " 'exploration',\n",
       " 'darker',\n",
       " 'themes',\n",
       " 'self-described',\n",
       " 'modern-day',\n",
       " 'feminist',\n",
       " 'creates',\n",
       " 'songs',\n",
       " 'that',\n",
       " 'are',\n",
       " 'often',\n",
       " 'characterized',\n",
       " 'relationships',\n",
       " 'monogamy',\n",
       " 'well',\n",
       " 'female',\n",
       " 'sexuality',\n",
       " 'empowerment',\n",
       " 'stage',\n",
       " 'dynamic',\n",
       " 'highly',\n",
       " 'choreographed',\n",
       " 'performances',\n",
       " 'have']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('200 separate words with latin letters from SQuAD')\n",
    "utils.get_word_list(squad_words_and_freqs_eng)[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary contains 104455 unique words with latin letters\n"
     ]
    }
   ],
   "source": [
    "print(\"The vocabulary contains {} unique words with latin letters\".format(len(squad_words_and_freqs_eng.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squad_words_and_freqs_eng_uncased = utils.remove_non_latin_words_from_map(squad_words_and_freqs_uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utils.save_dictionary_to_excel(squad_words_and_freqs_eng_uncased, \"../data/squad/word_freqs_eng_uncased1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary contains 90601 unique uncased words with latin letters\n"
     ]
    }
   ],
   "source": [
    "print(\"The vocabulary contains {} unique uncased words with latin letters\".format(len(squad_words_and_freqs_eng_uncased.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save all results to excel\n",
    "utils.save_dictionary_to_excel(squad_words_and_freqs, \"../data/squad/word_freqs.xlsx\")\n",
    "utils.save_dictionary_to_excel(squad_words_and_freqs_uncased, \"../data/squad/word_freqs_uncased.xlsx\")\n",
    "utils.save_dictionary_to_excel(squad_words_and_freqs_eng, \"../data/squad/word_freqs_eng.xlsx\")\n",
    "utils.save_dictionary_to_excel(squad_words_and_freqs_eng_uncased, \"../data/squad/word_freqs_eng_uncased.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings is a python package that provides pretrained word embeddings for natural language processing and machine learning.\n",
    "\n",
    "Instead of loading a large file to query for embeddings, embeddings is backed by a database and fast to load and query.\n",
    "\n",
    "Upon first use, the embeddings are first downloaded to disk in the form of a SQLite database. This may take a long time for large embeddings such as GloVe. Further usage of the embeddings are directly queried against the database. \n",
    "\n",
    "https://github.com/vzhong/embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from embeddings import GloveEmbedding, FastTextEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of word embedding comparison would be in result_df table. \n",
    "\n",
    "- name: short name od word embedding pretrained dataset \n",
    "- size(Mb): size of downloaded ptetrained model \n",
    "- number of words in training: total number of unique words in training dataset of pretrained model \n",
    "- number of unique tokens: total number of words in vocabulary of pretrained model \n",
    "- cased: are words in vocabulary cased \n",
    "- source of words: short name of source \n",
    "- made by: short name of author/company \n",
    "- missing words(%): percent of words from SQuAD dataset which are not in pretrained dataset \n",
    "- embedding coverage(%): percent of words from SQuAD dataset which are in pretrained dataset \n",
    "- missing words with latin letters %: percent of words with latin letters from SQuAD dataset which are not in pretrained dataset \n",
    "- embedding coverage with latin letters %: percent of words with latin letters from SQuAD dataset which are in pretrained dataset \n",
    "- first loading time(hours:min:sec): time for download pretrained model and fill sqlite database \n",
    "\n",
    "When every pretrained model is loaded the first time, loading time = time of downloading pretrained model + time of creating a database. When a model is reloaded, loading time is much less because we need only to load a database of vocabulary and, optionally, embedded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=['name', 'size(Mb)', 'number of words in training', 'number of unique tokens',\n",
    "                                   'cased', 'source of words', 'made by', 'missing words %', 'embedding coverage %',\n",
    "                                   'missing words with latin letters %', 'embedding coverage with latin letters %',\n",
    "                                   'first loading time hours/min/sec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fill this dataframe with: word, its frequency, existence in WE algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squad_words_and_freqs_df = pd.DataFrame(list(squad_words_and_freqs.items()), columns=['word', 'frequency'])\n",
    "squad_words_and_freqs_uncased_df =  pd.DataFrame(list(squad_words_and_freqs_uncased.items()), columns=['word', 'frequency'])\n",
    "squad_words_and_freqs_eng_df =  pd.DataFrame(list(squad_words_and_freqs_eng.items()), columns=['word', 'frequency'])\n",
    "squad_words_and_freqs_eng_uncased_df =  pd.DataFrame(list(squad_words_and_freqs_eng_uncased.items()), columns=['word', 'frequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe(Global Vectors for Word Representation) is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "Pre-trained word vectors:\n",
    "\n",
    "- Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download): glove.840B.300d.zip\n",
    "- Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download): glove.42B.300d.zip\n",
    "- Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download): glove.twitter.27B.zip\n",
    "- Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download): glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.126 GB download): glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://resources.wolframcloud.com/NeuralNetRepository/resources/GloVe-300-Dimensional-Word-Vectors-Trained-on-Common-Crawl-840B\n",
    "\n",
    "This model encodes 2,196,016 tokens as unique vectors, with all tokens outside the vocabulary encoded as the zero-vector. It was released in 2014 by the computer science department at Stanford University. This model uses Web data from Common Crawl, trained on 840 billion tokens, taking into account the case. All vectors are 300-dimensional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Time: 121.72 sec.\n"
     ]
    }
   ],
   "source": [
    "#initial load time= 1:46:38\n",
    "start_time = time.time()\n",
    "glove_common_crawl_840 = GloveEmbedding('common_crawl_840', d_emb=300, show_progress=True)\n",
    "print(\"Done. Time: {} sec.\".format(round(time.time()-start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 109584/109584 [15:08<00:00, 120.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of GloVe common_crawl_840 : 20171, percentage of successful word embedding 81.59311578332603%, unsuccessful 18.406884216673966%\n",
      "Done. Time: 908.6 sec.\n"
     ]
    }
   ],
   "source": [
    "words_out_of_model_glove_common_crawl_840 = utils.get_missing_words('GloVe common_crawl_840', glove_common_crawl_840, \n",
    "                                                                          utils.get_word_list(squad_words_and_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 109584/109584 [13:01<00:00, 140.25it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_df['GloVe common_crawl_840'] = utils.get_word_existence_for_WE(glove_common_crawl_840,\n",
    "                                                                                     utils.get_word_list(squad_words_and_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 104455/104455 [00:11<00:00, 9030.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of GloVe common_crawl_840 : 16374, percentage of successful word embedding 84.32435019865014%, unsuccessful 15.675649801349865%\n",
      "Done. Time: 11.57 sec.\n"
     ]
    }
   ],
   "source": [
    "eng_words_out_of_model_glove_common_crawl_840 = utils.get_missing_words('GloVe common_crawl_840', glove_common_crawl_840, \n",
    "                                                                          utils.get_word_list(squad_words_and_freqs_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 104455/104455 [00:11<00:00, 9153.36it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_eng_df['GloVe common_crawl_840'] = utils.get_word_existence_for_WE(glove_common_crawl_840,\n",
    "                                                                        utils.get_word_list(squad_words_and_freqs_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncovered word examples: ['bee-yon-say', 'Darlette', 'elsik', 'best-charting', '663000', '317000', '541000', '482000', 'number-ones', 'romance-themed', 'electro-r', 'Lifeandtimescom', 'TEDxEuston', 'goose-bump-inducing', 'diva-roars', 'irreemplazable', 'female-empowerment', 'man-tending', '11-motivated', 'mini-hula', 'Cooper-Donnell', 'beyontourage', 'Llewyn-Smith', 'Scaptia', 'beyonceae', 'CSPINET', 'GateFive', 'food-donation', \"itwasannouncedthatdestiny'swoulddisbaninwhatcity\", 'pareles']\n"
     ]
    }
   ],
   "source": [
    "print('uncovered word examples: {0}'.format(eng_words_out_of_model_glove_common_crawl_840[0:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squad_words_cnt = len(utils.get_word_list(squad_words_and_freqs))\n",
    "squad_words_en_cnt = len(utils.get_word_list(squad_words_and_freqs_eng))\n",
    "results_df.loc[len(results_df)]= ['GloVe common_crawl_840', 2126, '840 billion', '2 196 016', \n",
    "                                  'cased', 'Common Crawl', 'Stanford Univ',\n",
    "                                   (len(words_out_of_model_glove_common_crawl_840) / squad_words_cnt * 100),\n",
    "                                   (100 -  len(words_out_of_model_glove_common_crawl_840) / squad_words_cnt * 100), \n",
    "                                   (len(eng_words_out_of_model_glove_common_crawl_840) / squad_words_en_cnt * 100),\n",
    "                                   (100 -  len(eng_words_out_of_model_glove_common_crawl_840) / squad_words_en_cnt * 100), \n",
    "                                   '1:46:38']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save results with word existence\n",
    "squad_words_and_freqs_df.to_excel(\"../data/squad/word_freqs.xlsx\")\n",
    "squad_words_and_freqs_eng_df.to_excel(\"../data/squad/word_freqs_eng.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download): glove.42B.300d.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://resources.wolframcloud.com/NeuralNetRepository/resources/GloVe-300-Dimensional-Word-Vectors-Trained-on-Common-Crawl-42B\n",
    "\n",
    "This model encodes 1,917,495 tokens as unique vectors, with all tokens outside the vocabulary encoded as the zero-vector. It was released in 2014 by the computer science department at Stanford University. This model uses Web data from Common Crawl, trained on 42 billion tokens. All tokens are uncased. All vectors are 300-dimensional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Time: 152.65 sec.\n"
     ]
    }
   ],
   "source": [
    "#reload time\n",
    "start_time = time.time()\n",
    "glove_common_crawl_42 = GloveEmbedding('common_crawl_48', d_emb=300, show_progress=True)\n",
    "print(\"Done. Time: {} sec.\".format(round(time.time()-start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 95465/95465 [12:26<00:00, 127.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of GloVe common_crawl_42 : 15809, percentage of successful word embedding 83.44000419001728%, unsuccessful 16.559995809982716%\n",
      "Done. Time: 746.12 sec.\n"
     ]
    }
   ],
   "source": [
    "words_out_of_model_common_crawl_42 = utils.get_missing_words('GloVe common_crawl_42', glove_common_crawl_42, \n",
    "                                                                   utils.get_word_list(squad_words_and_freqs_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 95465/95465 [12:13<00:00, 130.07it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_uncased_df['GloVe common_crawl_42'] = utils.get_word_existence_for_WE(glove_common_crawl_42,\n",
    "                                                                         utils.get_word_list(squad_words_and_freqs_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 90601/90601 [00:09<00:00, 9694.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of GloVe common_crawl_42 : 12306, percentage of successful word embedding 86.41736846171676%, unsuccessful 13.582631538283241%\n",
      "Done. Time: 9.35 sec.\n"
     ]
    }
   ],
   "source": [
    "eng_words_out_of_model_common_crawl_42 = utils.get_missing_words('GloVe common_crawl_42', glove_common_crawl_42, \n",
    "                                                                   utils.get_word_list(squad_words_and_freqs_eng_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 90601/90601 [00:09<00:00, 9813.24it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_eng_uncased_df['GloVe common_crawl_42'] = utils.get_word_existence_for_WE(glove_common_crawl_42,\n",
    "                                                                         utils.get_word_list(squad_words_and_freqs_eng_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncovered word examples: ['bee-yon-say', '', 'darlette', 'best-charting', '663000', '317000', '541000', '482000', 'number-ones', 'electro-r', 'lifeandtimescom', 'tedxeuston', 'goose-bump-inducing', 'diva-roars', 'female-empowerment', 'man-tending', '11-motivated', 'mini-hula', 'cooper-donnell', 'beyontourage', 'llewyn-smith', 'scaptia', 'beyonceae', 'cspinet', 'gatefive', 'food-donation', \"itwasannouncedthatdestiny'swoulddisbaninwhatcity\", 'wholistedheratnumber17intheirlistoftop20hot100', 'advetisments', 'bayonce', 'whenwasitannouncedthatwasaco-ownerin', 'skarbeks', 'belweder', 'ursyn', 'niemcewicz', 'przebiegi', 'eolomelodicon', 'szafarnia', 'dziewanowski', 'salonik', 'mieroszewski', 'woyciechowski', 'nepomucen', 'witwicki', 'konstancja', 'piano-bashing', 'jachimecki', 'concert-giving', 'bendemann', 'amantine', 'mallefille', 'canuts', 'gestirne', 'fortune-hunting', 'bozzolini', 'prosseda', 'blanchar', 'methuen-campbell', 'obreskoff', 'czartoryska', 'cruveilhier', 'jeanne-anais', 'fioriture', 'ekier', 'cell-structure', 'zywny', '75-bar', 'note-values', 'mikuli', 'rubatos', 'ritardandos', 'kleofas', 'koczalski', '1837-847', 'gladkowska', 'composisition', 'geographicla', 'liszy', 'perforemed', 'biogrpahers', 'plitical', 'biogrpahies', 'euorpeans', 'oshow', 'polonasises', 'relude', 'gyaincain', 'ming-tibetan', 'phagpa', 'deshin', 'shekpa', 'mongol-tibetan', 'sino-an', 'priest-patron', 'myriarchies', 'phagmodru', 'myriarch', 'phagmodrupa', 'rolpe', 'zongluo', 'amdo-kham', 'army-civilian', 'wanhu', 'qianhu', 'chiliarchies', 'tieh-tseng', 'lama-patron', 'tsepon', 'shakabpa', 'punala', 'ex-yuan', 'khanbaliq', 'tibetologist', 'ethno-geographic', 'ayurbarwada', 'buyantu', 'choskunskyabs', 'qingying', 'phachu', 'kargyu', 'neiwo', 'renbam', 'gtsang', 'bielenstein', 'degsi', 'sde-srid', 'lok-ham', 'dpon', 'sagya', 'chosrje', 'yeshes', 'tribute-cum-trade', 'yung-chin', 'shih-shan', 'myriarchy', 'rinpungpa', 'tsangpa', 'guangxiao', 'linggu', 'hok-lam', 'skekpa', 'tribute-related', 'tribute-bearers', 'taozhou', 'petech', 'rinbung', 'censorate', 'tinghe', 'huckenpahler', 'juzheng', 'dorjichang', 'soinam', 'lozui', 'tenkyong', 'lozang', 'rapten', 'gelugpas', 'khoshut', 'qoshot', 'dzungar', 'whatdidclaimchangchubgyaltsenwantedtoremove', 'whowastheviceroyaltyoftheregimeestablishedby', 'burstcom', 'dandumont', 'dual-transistor', 'capacitor-coupled', 'rival-drm', 'clickwheel-based', 'end-of-the-decade', 'mp3com', 'previously-patented', '6th-generation', 'higher-than-allowed', 'pat-rights', 'towairaito', 'purinsesu', '1upcom', 'l-targeting', 'projectile-based', 'imp-like', 'pseudo-speech', 'bulblins', 'twilight-covered', 'pointing-based', 'controller-functionality', 'amiibo', 'remote-swinging', 'mangaone', 'bulbins', 'originally-planned', 'privately-backed', 'non-eon', 'clapperboards', '790000', 'rogerebertcom', 'oddjob-like', 'psycho-villain', \"l'americain\", 'c-x75s', 'db10s']\n"
     ]
    }
   ],
   "source": [
    "print('uncovered word examples: {0}'.format(eng_words_out_of_model_common_crawl_42[0:200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: \n",
    "- even after removing words which contain non-latin letters there are still some non-english words written with English letters, like 'literaturna'(Bulgarian), 'hongzhu'(Chinese). \n",
    "- there are some words with typo errors, like 'culturural'(maby there should be 'cultural'), 'desagreements' (maby there should be 'disagreements'), 'percentsge'(maby there should be 'percentage'). \n",
    "- besides this, it seems that word embedding algorithms can't encode float numbers like '638,817' or big numbers with ',' as separator like '1,698,465'- maby such numbers should be sepatared by ','. \n",
    "- there are some custom constructions like '50-xxxx', 'emf2', '20-cwt', which are unknown in dictionary. \n",
    "- there are some rare English words, like 'intendancy', 'anti-monarchists', which are not covered \n",
    "- there are some names  which were not marked as named entities, like 'Vseslav', 'Demira'\n",
    "- there are some incorrectly parsed words like '520-485', 'n=279', '9,192,631,770'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squad_words_cnt = len(utils.get_word_list(squad_words_and_freqs_uncased))\n",
    "squad_words_en_cnt = len(utils.get_word_list(squad_words_and_freqs_eng_uncased))\n",
    "results_df.loc[len(results_df)] = ['GloVe common_crawl_42', 1833, '42 billion', '1 917 495', \n",
    "                                    'uncased', 'Common Crawl', 'Stanford Univ',\n",
    "                                    len(words_out_of_model_common_crawl_42) / squad_words_cnt * 100,\n",
    "                                    100 -  len(words_out_of_model_common_crawl_42) / squad_words_cnt * 100,\n",
    "                                    len(eng_words_out_of_model_common_crawl_42) / squad_words_en_cnt * 100,\n",
    "                                    100 -  len(eng_words_out_of_model_common_crawl_42) / squad_words_en_cnt * 100,\n",
    "                                    '1:30:10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save results with word existence\n",
    "squad_words_and_freqs_uncased_df.to_excel(\"../data/squad/word_freqs_uncased.xlsx\")\n",
    "squad_words_and_freqs_eng_uncased_df.to_excel(\"../data/squad/word_freqs_eng_uncased.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download): glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://resources.wolframcloud.com/NeuralNetRepository/resources/GloVe-200-Dimensional-Word-Vectors-Trained-on-Tweets\n",
    "\n",
    "This model encodes 1,193,515 tokens as unique vectors, with all tokens outside the vocabulary encoded as the zero-vector. It was released in 2014 by the computer science department at Stanford University. This model uses Tweets, trained on 27 billion tweets. All tokens are uncased. All vectors are 200-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Time: 57.21 sec.\n"
     ]
    }
   ],
   "source": [
    "#reload time\n",
    "start_time = time.time()\n",
    "glove_twitter = GloveEmbedding('twitter', d_emb=200, show_progress=True)\n",
    "print(\"Done. Time: {} sec.\".format(round(time.time()-start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 95465/95465 [07:33<00:00, 210.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of GloVe twitter : 44375, percentage of successful word embedding 53.516995757607496%, unsuccessful 46.483004242392504%\n",
      "Done. Time: 453.14 sec.\n"
     ]
    }
   ],
   "source": [
    "words_out_of_model_glove_twitter = utils.get_missing_words('GloVe twitter', glove_twitter, \n",
    "                                                           utils.get_word_list(squad_words_and_freqs_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 95465/95465 [07:47<00:00, 204.39it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_uncased_df['GloVe twitter'] = utils.get_word_existence_for_WE(glove_twitter,\n",
    "                                                                         utils.get_word_list(squad_words_and_freqs_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 90601/90601 [00:07<00:00, 11582.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of GloVe twitter : 40749, percentage of successful word embedding 55.02367523537268%, unsuccessful 44.97632476462732%\n",
      "Done. Time: 7.83 sec.\n"
     ]
    }
   ],
   "source": [
    "eng_words_out_of_model_glove_twitter = utils.get_missing_words('GloVe twitter', glove_twitter, \n",
    "                                                               utils.get_word_list(squad_words_and_freqs_eng_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 90601/90601 [00:07<00:00, 11981.99it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_eng_uncased_df['GloVe twitter'] = utils.get_word_existence_for_WE(glove_twitter,\n",
    "                                                                     utils.get_word_list(squad_words_and_freqs_eng_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncovered word examples: ['bee-yon-say', '4', '1981', '1990s', 'girl-group', '2003', '100', 'disbandment', '2005', '2006', '2009', '2008', '2010', '2011', 'mellower', '1970s', '1980s', '2013', '19', '118', '60', '20', '2000s', '2014', '2015', 'darlette', '15', '16-year-olds', '1990', 'frager']\n"
     ]
    }
   ],
   "source": [
    "print('uncovered word examples: {0}'.format(eng_words_out_of_model_glove_twitter[0:30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the list of uncovered word there is \"creek),[citation\". It means that training set could be splitted to separate words better if we add some regular expressions to string tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squad_words_cnt = len(utils.get_word_list(squad_words_and_freqs_uncased))\n",
    "squad_words_en_cnt = len(utils.get_word_list(squad_words_and_freqs_eng_uncased))\n",
    "results_df.loc[len(results_df)] = ['GloVe twitter', 1484, '27 billion', '1 193 515', \n",
    "                                    'uncased', 'Tweets', 'Stanford Univ',\n",
    "                                    len(words_out_of_model_glove_twitter) / squad_words_cnt * 100,\n",
    "                                    100 -  len(words_out_of_model_glove_twitter) / squad_words_cnt * 100,\n",
    "                                    len(eng_words_out_of_model_glove_twitter) / squad_words_en_cnt * 100,\n",
    "                                    100 -  len(eng_words_out_of_model_glove_twitter) / squad_words_en_cnt * 100,\n",
    "                                    '00:27:06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save results with word existence\n",
    "squad_words_and_freqs_uncased_df.to_excel(\"../data/squad/word_freqs_uncased.xlsx\")\n",
    "squad_words_and_freqs_eng_uncased_df.to_excel(\"../data/squad/word_freqs_eng_uncased.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download): glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://resources.wolframcloud.com/NeuralNetRepository/resources/GloVe-300-Dimensional-Word-Vectors-Trained-on-Wikipedia-and-Gigaword-5-Data\n",
    "\n",
    "This model encodes 400,000 tokens as unique vectors, with all tokens outside the vocabulary encoded as the zero-vector. It was released in 2014 by the computer science department at Stanford University. This model uses a combination of the Wikipedia 2014 dump and the Gigaword 5 corpus, trained on 6 billion tokens. All tokens are uncased. All vectors are 200-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Time: 16.14 sec.\n"
     ]
    }
   ],
   "source": [
    "#reload time\n",
    "start_time = time.time()\n",
    "glove_wikipedia_gigaword = GloveEmbedding('wikipedia_gigaword', d_emb=300, show_progress=True)\n",
    "print(\"Done. Time: {} sec.\".format(round(time.time()-start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 95465/95465 [06:37<00:00, 240.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of GloVe wikipedia_gigaword : 24175, percentage of successful word embedding 74.67658304090504%, unsuccessful 25.323416959094956%\n",
      "Done. Time: 397.6 sec.\n"
     ]
    }
   ],
   "source": [
    "words_out_of_model_glove_wikipedia_gigaword = utils.get_missing_words('GloVe wikipedia_gigaword', \n",
    "                                                glove_wikipedia_gigaword, utils.get_word_list(squad_words_and_freqs_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 95465/95465 [08:53<00:00, 178.95it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_uncased_df['GloVe wikipedia_gigaword'] = utils.get_word_existence_for_WE(glove_wikipedia_gigaword,\n",
    "                                                         utils.get_word_list(squad_words_and_freqs_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 90601/90601 [00:08<00:00, 11014.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of GloVe wikipedia_gigaword : 20576, percentage of successful word embedding 77.28943389145815%, unsuccessful 22.710566108541848%\n",
      "Done. Time: 8.27 sec.\n"
     ]
    }
   ],
   "source": [
    "eng_words_out_of_model_glove_wikipedia_gigaword = utils.get_missing_words('GloVe wikipedia_gigaword', \n",
    "                                              glove_wikipedia_gigaword, utils.get_word_list(squad_words_and_freqs_eng_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 90601/90601 [00:07<00:00, 11398.57it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_eng_uncased_df['GloVe wikipedia_gigaword'] = utils.get_word_existence_for_WE(glove_wikipedia_gigaword,\n",
    "                                                                 utils.get_word_list(squad_words_and_freqs_eng_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squad_words_cnt = len(utils.get_word_list(squad_words_and_freqs_uncased))\n",
    "squad_words_en_cnt = len(utils.get_word_list(squad_words_and_freqs_eng_uncased))\n",
    "results_df.loc[len(results_df)] = ['GloVe wikipedia_gigaword', 841, '6 billion', '400 000', \n",
    "                                    'uncased', 'Wikipedia 2014 + Gigaword 5', 'Stanford Univ',\n",
    "                                    len(words_out_of_model_glove_wikipedia_gigaword) / squad_words_cnt * 100,\n",
    "                                    100 -  len(words_out_of_model_glove_wikipedia_gigaword) / squad_words_cnt * 100,\n",
    "                                    len(eng_words_out_of_model_glove_wikipedia_gigaword) / squad_words_en_cnt * 100,\n",
    "                                    100 -  len(eng_words_out_of_model_glove_wikipedia_gigaword) / squad_words_en_cnt * 100,\n",
    "                                    '00:03:58']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size(Mb)</th>\n",
       "      <th>number of words in training</th>\n",
       "      <th>number of unique tokens</th>\n",
       "      <th>cased</th>\n",
       "      <th>source of words</th>\n",
       "      <th>made by</th>\n",
       "      <th>missing words %</th>\n",
       "      <th>embedding coverage %</th>\n",
       "      <th>missing words with latin letters %</th>\n",
       "      <th>embedding coverage with latin letters %</th>\n",
       "      <th>first loading time hours/min/sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GloVe common_crawl_840</td>\n",
       "      <td>2126</td>\n",
       "      <td>840 billion</td>\n",
       "      <td>2 196 016</td>\n",
       "      <td>cased</td>\n",
       "      <td>Common Crawl</td>\n",
       "      <td>Stanford Univ</td>\n",
       "      <td>18.406884</td>\n",
       "      <td>81.593116</td>\n",
       "      <td>15.675650</td>\n",
       "      <td>84.324350</td>\n",
       "      <td>1:46:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GloVe common_crawl_42</td>\n",
       "      <td>1833</td>\n",
       "      <td>42 billion</td>\n",
       "      <td>1 917 495</td>\n",
       "      <td>uncased</td>\n",
       "      <td>Common Crawl</td>\n",
       "      <td>Stanford Univ</td>\n",
       "      <td>16.559996</td>\n",
       "      <td>83.440004</td>\n",
       "      <td>13.582632</td>\n",
       "      <td>86.417368</td>\n",
       "      <td>1:30:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GloVe twitter</td>\n",
       "      <td>1484</td>\n",
       "      <td>27 billion</td>\n",
       "      <td>1 193 515</td>\n",
       "      <td>uncased</td>\n",
       "      <td>Tweets</td>\n",
       "      <td>Stanford Univ</td>\n",
       "      <td>46.483004</td>\n",
       "      <td>53.516996</td>\n",
       "      <td>44.976325</td>\n",
       "      <td>55.023675</td>\n",
       "      <td>00:27:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GloVe wikipedia_gigaword</td>\n",
       "      <td>841</td>\n",
       "      <td>6 billion</td>\n",
       "      <td>400 000</td>\n",
       "      <td>uncased</td>\n",
       "      <td>Wikipedia 2014 + Gigaword 5</td>\n",
       "      <td>Stanford Univ</td>\n",
       "      <td>25.323417</td>\n",
       "      <td>74.676583</td>\n",
       "      <td>22.710566</td>\n",
       "      <td>77.289434</td>\n",
       "      <td>00:03:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name size(Mb) number of words in training  \\\n",
       "0    GloVe common_crawl_840     2126                 840 billion   \n",
       "1     GloVe common_crawl_42     1833                  42 billion   \n",
       "2             GloVe twitter     1484                  27 billion   \n",
       "3  GloVe wikipedia_gigaword      841                   6 billion   \n",
       "\n",
       "  number of unique tokens    cased              source of words  \\\n",
       "0               2 196 016    cased                 Common Crawl   \n",
       "1               1 917 495  uncased                 Common Crawl   \n",
       "2               1 193 515  uncased                       Tweets   \n",
       "3                 400 000  uncased  Wikipedia 2014 + Gigaword 5   \n",
       "\n",
       "         made by  missing words %  embedding coverage %  \\\n",
       "0  Stanford Univ        18.406884             81.593116   \n",
       "1  Stanford Univ        16.559996             83.440004   \n",
       "2  Stanford Univ        46.483004             53.516996   \n",
       "3  Stanford Univ        25.323417             74.676583   \n",
       "\n",
       "   missing words with latin letters %  \\\n",
       "0                           15.675650   \n",
       "1                           13.582632   \n",
       "2                           44.976325   \n",
       "3                           22.710566   \n",
       "\n",
       "   embedding coverage with latin letters % first loading time hours/min/sec  \n",
       "0                                84.324350                          1:46:38  \n",
       "1                                86.417368                          1:30:10  \n",
       "2                                55.023675                         00:27:06  \n",
       "3                                77.289434                         00:03:58  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save results with word existence\n",
    "squad_words_and_freqs_uncased_df.to_excel(\"../data/squad/word_freqs_uncased.xlsx\")\n",
    "squad_words_and_freqs_eng_uncased_df.to_excel(\"../data/squad/word_freqs_eng_uncased.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe Wikipedia gigaword was first time loaded faster than other models because of less downloading size (less than 1Gb) and less number of words in vocabulary (we have to prepare only 400 000 records in the database in comparing to more than 1 million for other datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>123</th>\n",
       "      <th>3</th>\n",
       "      <th>cyclotron</th>\n",
       "      <th>Knowles-Carter</th>\n",
       "      <th>New</th>\n",
       "      <th>York</th>\n",
       "      <th>New York</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>crawl_840</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>not in dict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crawl_42</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>not in dict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>twitter</td>\n",
       "      <td>not in dict</td>\n",
       "      <td>not in dict</td>\n",
       "      <td>not in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>not in dict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>not in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>not in dict</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name          123            3    cyclotron Knowles-Carter      New  \\\n",
       "0  crawl_840      in dict      in dict      in dict        in dict  in dict   \n",
       "1   crawl_42      in dict      in dict      in dict        in dict  in dict   \n",
       "2    twitter  not in dict  not in dict  not in dict        in dict  in dict   \n",
       "3  wikipedia      in dict      in dict      in dict    not in dict  in dict   \n",
       "\n",
       "      York     New York  \n",
       "0  in dict  not in dict  \n",
       "1  in dict  not in dict  \n",
       "2  in dict  not in dict  \n",
       "3  in dict  not in dict  "
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words = ['123', '3', 'cyclotron', 'Knowles-Carter', 'New', 'York', 'New York']\n",
    "test_words_lowercased = ['123', '3', 'cyclotron', 'knowles-carter', 'new', 'york', 'new york']\n",
    "glove_model_names = ['crawl_840','crawl_42','twitter','wikipedia']\n",
    "glove_model_case = ['cased','uncased','uncased','uncased']\n",
    "glove_models = [glove_common_crawl_840, glove_common_crawl_42, glove_twitter, glove_wikipedia_gigaword]\n",
    "\n",
    "test_df = utils.process_test_words(glove_model_names, glove_model_case, test_words, test_words_lowercased, glove_models)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best pretrained model among GloVe models is GloVe common_crawl_42\tas it has the less percentage of uncovered SQuAD words. Little percent of covered words for wikipedia could be connected with the fact that all numbers and some rare words are uncovered by this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fastText word embedding is contributed by the same group of people who established word2vec. It extends word2vec by introducing subword modeling. It represents each word was a bag of character n-gram.\n",
    "\n",
    "Fasttext is a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations.\n",
    "\n",
    "For .bin use: load_fasttext_format() (this typically contains a full model with parameters, ngrams, etc.).\n",
    "\n",
    "For .vec use: load_word2vec_format (this contains ONLY word-vectors -> no ngrams + you can't update a model).\n",
    "\n",
    "https://fasttext.cc/docs/en/english-vectors.html\n",
    "\n",
    "Pre-trained word vectors learned on different sources can be downloaded below:\n",
    "\n",
    "- Wiki word vectors\n",
    "- wiki-news-300d-1M.vec.zip: 1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).\n",
    "- wiki-news-300d-1M-subword.vec.zip: 1 million word vectors trained with subword infomation on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).\n",
    "- crawl-300d-2M.vec.zip: 2 million word vectors trained on Common Crawl (600B tokens).\n",
    "- crawl-300d-2M-subword.zip: 2 million word vectors trained with subword information on Common Crawl (600B tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiki word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://fasttext.cc/docs/en/pretrained-vectors.html\n",
    "    \n",
    "Vectors trained on Wikipedia using fastText. \n",
    "These vectors in dimension 300 were obtained using the skip-gram model described in Bojanowski et al. (2016) \n",
    "with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Time: 148.07 sec.\n"
     ]
    }
   ],
   "source": [
    "#reload time\n",
    "start_time = time.time()\n",
    "fasttext_wiki = FastTextEmbedding()\n",
    "print(\"Done. Time: {} sec.\".format(round(time.time()-start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 95465/95465 [10:17<00:00, 154.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of Fasttext fasttext_wiki : 21531, percentage of successful word embedding 77.44618446551092%, unsuccessful 22.55381553448908%\n",
      "Done. Time: 617.6 sec.\n"
     ]
    }
   ],
   "source": [
    "words_out_of_model_fasttext_wiki = utils.get_missing_words('Fasttext fasttext_wiki', \n",
    "                                                            fasttext_wiki, utils.get_word_list(squad_words_and_freqs_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 95465/95465 [11:33<00:00, 137.57it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_uncased_df['Fasttext fasttext_wiki'] = utils.get_word_existence_for_WE(fasttext_wiki,\n",
    "                                                         utils.get_word_list(squad_words_and_freqs_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 90601/90601 [00:07<00:00, 11922.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of Fasttext fasttext_wiki : 19286, percentage of successful word embedding 78.71325923554927%, unsuccessful 21.286740764450723%\n",
      "Done. Time: 7.61 sec.\n"
     ]
    }
   ],
   "source": [
    "eng_words_out_of_model_fasttext_wiki = utils.get_missing_words('Fasttext fasttext_wiki', \n",
    "                                                    fasttext_wiki,  utils.get_word_list(squad_words_and_freqs_eng_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 90601/90601 [00:07<00:00, 11545.28it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_eng_uncased_df['Fasttext fasttext_wiki'] = utils.get_word_existence_for_WE(fasttext_wiki,\n",
    "                                                                 utils.get_word_list(squad_words_and_freqs_eng_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squad_words_cnt = len(utils.get_word_list(squad_words_and_freqs_uncased))\n",
    "squad_words_en_cnt = len(utils.get_word_list(squad_words_and_freqs_eng_uncased))\n",
    "results_df.loc[len(results_df)] = ['fasttext fasttext_wiki', 10114, 'more than 100 millions', '2 518 927', \n",
    "                                    'uncased', 'Wikipedia', 'P. Bojanowski*, E. Grave, A. Joulin, T. Mikolov',\n",
    "                                    len(words_out_of_model_fasttext_wiki) / squad_words_cnt * 100,\n",
    "                                    100 -  len(words_out_of_model_fasttext_wiki) / squad_words_cnt * 100,\n",
    "                                    len(eng_words_out_of_model_fasttext_wiki) / squad_words_en_cnt * 100,\n",
    "                                    100 -  len(eng_words_out_of_model_fasttext_wiki) / squad_words_en_cnt * 100,\n",
    "                                    '12:10:02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save results with word existence\n",
    "squad_words_and_freqs_uncased_df.to_excel(\"../data/squad/word_freqs_uncased.xlsx\")\n",
    "squad_words_and_freqs_eng_uncased_df.to_excel(\"../data/squad/word_freqs_eng_uncased.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from word_emb import Word_Emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wiki-news-300d-1M.vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://fasttext.cc/docs/en/english-vectors.html\n",
    "\n",
    "1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from fasttext_emb import FastTextEmbedding_All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Time: 46.59 sec.\n"
     ]
    }
   ],
   "source": [
    "#reload time\n",
    "start_time = time.time()\n",
    "#fasttext_wiki_news = FastTextEmbedding_All(name='wiki_news', d_emb=300)\n",
    "fasttext_wiki_news = Word_Emb(url='https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki-news-300d-1M.vec.zip', \n",
    "                     emb_alg = 'fasttext', short_name = 'wiki_news', vec_name = 'wiki-news-300d-1M.vec')\n",
    "print(\"Done. Time: {} sec.\".format(round(time.time()-start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 109584/109584 [11:50<00:00, 154.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of Fasttext fasttext_wiki_news : 21392, percentage of successful word embedding 80.47890202949336%, unsuccessful 19.52109797050664%\n",
      "Done. Time: 710.57 sec.\n"
     ]
    }
   ],
   "source": [
    "words_out_of_model_fasttext_wiki_news = utils.get_missing_words('Fasttext fasttext_wiki_news', \n",
    "                                                           fasttext_wiki_news, utils.get_word_list(squad_words_and_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 109584/109584 [09:42<00:00, 188.03it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_df['Fasttext fasttext_wiki_news'] = utils.get_word_existence_for_WE(fasttext_wiki_news,\n",
    "                                                                                     utils.get_word_list(squad_words_and_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 104455/104455 [00:09<00:00, 10636.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of Fasttext fasttext_wiki_news : 18009, percentage of successful word embedding 82.75908285864726%, unsuccessful 17.240917141352735%\n",
      "Done. Time: 9.83 sec.\n"
     ]
    }
   ],
   "source": [
    "eng_words_out_of_model_fasttext_wiki_news = utils.get_missing_words('Fasttext fasttext_wiki_news', \n",
    "                                                       fasttext_wiki_news, utils.get_word_list(squad_words_and_freqs_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 104455/104455 [00:10<00:00, 9992.25it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_eng_df['Fasttext fasttext_wiki_news'] = utils.get_word_existence_for_WE(fasttext_wiki_news,\n",
    "                                                                         utils.get_word_list(squad_words_and_freqs_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squad_words_cnt = len(utils.get_word_list(squad_words_and_freqs))\n",
    "squad_words_en_cnt = len(utils.get_word_list(squad_words_and_freqs_eng))\n",
    "results_df.loc[len(results_df)] = ['fasttext fasttext_wiki_news', 665, '16 billion', '1 million', \n",
    "                                    'cased', 'Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset',\n",
    "                                    'T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, A. Joulin',\n",
    "                                    len(words_out_of_model_fasttext_wiki_news) / squad_words_cnt * 100,\n",
    "                                    100 -  len(words_out_of_model_fasttext_wiki_news) / squad_words_cnt * 100,\n",
    "                                    len(eng_words_out_of_model_fasttext_wiki_news) / squad_words_en_cnt * 100,\n",
    "                                    100 -  len(eng_words_out_of_model_fasttext_wiki_news) / squad_words_en_cnt * 100,\n",
    "                                    '00:29:42']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save results with word existence\n",
    "squad_words_and_freqs_df.to_excel(\"../data/squad/word_freqs.xlsx\")\n",
    "squad_words_and_freqs_eng_df.to_excel(\"../data/squad/word_freqs_eng.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wiki-news-300d-1M-subword.vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subowrd information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard word vectors ignore word internal structure that contains rich information. This information could be useful\n",
    "for computing representations of rare or mispelled words.\n",
    "A simple yet effective approach is to enrich the word vectors with a bag of character n-gram vectors that is either derived from the singular value decomposition of the co-occurence matrix (Sch ¨utze, 1993) or directly learned from a large corpus of data (Bojanowski et al., 2017).\n",
    "\n",
    "https://arxiv.org/pdf/1712.09405.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://fasttext.cc/docs/en/english-vectors.html\n",
    "\n",
    "1 million word vectors trained with subword infomation on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Time: 45.24 sec.\n"
     ]
    }
   ],
   "source": [
    "#reload time\n",
    "start_time = time.time()\n",
    "#fasttext_wiki_news_subword = FastTextEmbedding_All(name='wiki_news_subword', d_emb=300)\n",
    "fasttext_wiki_news_subword = Word_Emb(url='https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki-news-300d-1M-subword.vec.zip', \n",
    "                     emb_alg = 'fasttext', short_name = 'wiki_news_subword', vec_name = 'wiki-news-300d-1M-subword.vec')\n",
    "print(\"Done. Time: {} sec.\".format(round(time.time()-start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 109581/109581 [10:53<00:00, 167.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of Fasttext fasttext_wiki_news_subword : 21393, percentage of successful word embedding 80.47745503326307%, unsuccessful 19.522544966736934%\n",
      "Done. Time: 653.52 sec.\n"
     ]
    }
   ],
   "source": [
    "words_out_of_model_fasttext_wiki_news_subword = utils.get_missing_words('Fasttext fasttext_wiki_news_subword', \n",
    "                                                       fasttext_wiki_news_subword, utils.get_word_list(squad_words_and_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 109584/109584 [10:13<00:00, 178.48it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_df['Fasttext fasttext_wiki_news_subword'] = utils.get_word_existence_for_WE(fasttext_wiki_news_subword,\n",
    "                                                                         utils.get_word_list(squad_words_and_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 104835/104835 [00:09<00:00, 10601.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of Fasttext fasttext_wiki_news_subword : 17864, percentage of successful word embedding 82.95988934993085%, unsuccessful 17.040110650069156%\n",
      "Done. Time: 9.89 sec.\n"
     ]
    }
   ],
   "source": [
    "eng_words_out_of_model_fasttext_wiki_news_subword = utils.get_missing_words('Fasttext fasttext_wiki_news_subword', \n",
    "                                                       fasttext_wiki_news_subword, utils.get_word_list(squad_words_and_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 104455/104455 [00:09<00:00, 10935.93it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_eng_df['Fasttext fasttext_wiki_news_subword'] = utils.get_word_existence_for_WE(fasttext_wiki_news_subword,\n",
    "                                                                         utils.get_word_list(squad_words_and_freqs_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squad_words_cnt = len(utils.get_word_list(squad_words_and_freqs))\n",
    "squad_words_en_cnt = len(utils.get_word_list(squad_words_and_freqs_eng))\n",
    "results_df.loc[len(results_df)] = ['fasttext fasttext_wiki_news_subword', 574, '16 billion', '1 million', \n",
    "                                    'cased', 'Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset',\n",
    "                                    'T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, A. Joulin',\n",
    "                                    len(words_out_of_model_fasttext_wiki_news_subword) / squad_words_cnt * 100,\n",
    "                                    100 -  len(words_out_of_model_fasttext_wiki_news_subword) / squad_words_cnt * 100,\n",
    "                                    len(eng_words_out_of_model_fasttext_wiki_news_subword) / squad_words_en_cnt * 100,\n",
    "                                    100 -  len(eng_words_out_of_model_fasttext_wiki_news_subword) / squad_words_en_cnt * 100,\n",
    "                                    '00:27:12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save results with word existence\n",
    "squad_words_and_freqs_df.to_excel(\"../data/squad/word_freqs.xlsx\")\n",
    "squad_words_and_freqs_eng_df.to_excel(\"../data/squad/word_freqs_eng.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### crawl-300d-2M.vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 million word vectors trained on Common Crawl (600B tokens).\n",
    "\n",
    "The source of text data for this model the common crawl. While they provide noisier data than Wikipedia articles, they come in larger amounts and with a broader coverage. (http://www.lrec-conf.org/proceedings/lrec2018/pdf/627.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Time: 136.82 sec.\n"
     ]
    }
   ],
   "source": [
    "#reload time\n",
    "start_time = time.time()\n",
    "fasttext_crawl = Word_Emb(url='https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl-300d-2M.vec.zip', \n",
    "               emb_alg = 'fasttext', short_name = 'crawl', vec_name = 'crawl-300d-2M.vec')\n",
    "print(\"Done. Time: {} sec.\".format(round(time.time()-start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 109584/109584 [12:59<00:00, 140.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of Fasttext fasttext_crawl : 20018, percentage of successful word embedding 81.73273470579647%, unsuccessful 18.267265294203533%\n",
      "Done. Time: 779.47 sec.\n"
     ]
    }
   ],
   "source": [
    "words_out_of_model_fasttext_crawl = utils.get_missing_words('Fasttext fasttext_crawl', fasttext_crawl, \n",
    "                                                            utils.get_word_list(squad_words_and_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 109584/109584 [00:09<00:00, 11362.27it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_df['Fasttext fasttext_crawl'] = utils.get_word_existence_for_WE(fasttext_crawl,\n",
    "                                                                         utils.get_word_list(squad_words_and_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 104835/104835 [00:09<00:00, 10875.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of Fasttext fasttext_crawl : 16425, percentage of successful word embedding 84.33252253541279%, unsuccessful 15.66747746458721%\n",
      "Done. Time: 9.65 sec.\n"
     ]
    }
   ],
   "source": [
    "eng_words_out_of_model_fasttext_crawl = utils.get_missing_words('Fasttext fasttext_crawl', fasttext_crawl, \n",
    "                                                                utils.get_word_list(squad_words_and_freqs_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 104455/104455 [00:09<00:00, 11240.75it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_eng_df['Fasttext fasttext_crawl'] = utils.get_word_existence_for_WE(fasttext_crawl,\n",
    "                                                                         utils.get_word_list(squad_words_and_freqs_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squad_words_cnt = len(utils.get_word_list(squad_words_and_freqs))\n",
    "squad_words_en_cnt = len(utils.get_word_list(squad_words_and_freqs_eng))\n",
    "results_df.loc[len(results_df)] = ['fasttext crawl', 1488, '600 billion', '2 million', \n",
    "                                    'cased', 'Common Crawl ',\n",
    "                                    'T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, A. Joulin',\n",
    "                                    len(words_out_of_model_fasttext_crawl) / squad_words_cnt * 100,\n",
    "                                    100 -  len(words_out_of_model_fasttext_crawl) / squad_words_cnt * 100,\n",
    "                                    len(eng_words_out_of_model_fasttext_crawl) / squad_words_en_cnt * 100,\n",
    "                                    100 -  len(eng_words_out_of_model_fasttext_crawl) / squad_words_en_cnt * 100,\n",
    "                                    '1:43:49']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save results with word existence\n",
    "squad_words_and_freqs_df.to_excel(\"../data/squad/word_freqs.xlsx\")\n",
    "squad_words_and_freqs_eng_df.to_excel(\"../data/squad/word_freqs_eng.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### crawl-300d-2M-subword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 million word vectors trained with subword information on Common Crawl (600B tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Time: 145.81 sec.\n"
     ]
    }
   ],
   "source": [
    "#reload time (initial load time= 3:50:24)\n",
    "start_time = time.time()\n",
    "fasttext_crawl_subword = Word_Emb(url='https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl-300d-2M-subword.zip', \n",
    "               emb_alg = 'fasttext', short_name = 'crawl-subword', vec_name = 'crawl-300d-2M-subword.vec')\n",
    "print(\"Done. Time: {} sec.\".format(round(time.time()-start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 109964/109964 [15:23<00:00, 119.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of Fasttext fasttext_crawl_subword : 19842, percentage of successful word embedding 81.95591284420355%, unsuccessful 18.044087155796444%\n",
      "Done. Time: 923.75 sec.\n"
     ]
    }
   ],
   "source": [
    "words_out_of_model_fasttext_crawl_subword = utils.get_missing_words('Fasttext fasttext_crawl_subword', \n",
    "                                                      fasttext_crawl_subword, utils.get_word_list(squad_words_and_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 109584/109584 [15:33<00:00, 117.35it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_df['Fasttext fasttext_crawl_subword'] = utils.get_word_existence_for_WE(fasttext_crawl_subword,\n",
    "                                                                         utils.get_word_list(squad_words_and_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 104835/104835 [00:09<00:00, 10988.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of Fasttext fasttext_crawl_subword : 16287, percentage of successful word embedding 84.46415796251252%, unsuccessful 15.53584203748748%\n",
      "Done. Time: 9.55 sec.\n"
     ]
    }
   ],
   "source": [
    "eng_words_out_of_model_fasttext_crawl_subword = utils.get_missing_words('Fasttext fasttext_crawl_subword', \n",
    "                                                  fasttext_crawl_subword,  utils.get_word_list(squad_words_and_freqs_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 104455/104455 [00:09<00:00, 11240.75it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_eng_df['Fasttext fasttext_crawl_subword'] = utils.get_word_existence_for_WE(fasttext_crawl_subword,\n",
    "                                                                         utils.get_word_list(squad_words_and_freqs_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squad_words_cnt = len(utils.get_word_list(squad_words_and_freqs))\n",
    "squad_words_en_cnt = len(utils.get_word_list(squad_words_and_freqs_eng))\n",
    "results_df.loc[len(results_df)] = ['fasttext fasttext_crawl_subword', 5691, '600 billion', '2 million', \n",
    "                                    'cased', 'Common Crawl ',\n",
    "                                    'T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, A. Joulin',\n",
    "                                    len(words_out_of_model_fasttext_crawl_subword) / squad_words_cnt * 100,\n",
    "                                    100 -  len(words_out_of_model_fasttext_crawl_subword) / squad_words_cnt * 100,\n",
    "                                    len(eng_words_out_of_model_fasttext_crawl_subword) / squad_words_en_cnt * 100,\n",
    "                                    100 -  len(eng_words_out_of_model_fasttext_crawl_subword) / squad_words_en_cnt * 100,\n",
    "                                    '3:50:24']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save results with word existence\n",
    "squad_words_and_freqs_df.to_excel(\"../data/squad/word_freqs.xlsx\")\n",
    "squad_words_and_freqs_eng_df.to_excel(\"../data/squad/word_freqs_eng.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size(Mb)</th>\n",
       "      <th>number of words in training</th>\n",
       "      <th>number of unique tokens</th>\n",
       "      <th>cased</th>\n",
       "      <th>source of words</th>\n",
       "      <th>made by</th>\n",
       "      <th>missing words %</th>\n",
       "      <th>embedding coverage %</th>\n",
       "      <th>missing words with latin letters %</th>\n",
       "      <th>embedding coverage with latin letters %</th>\n",
       "      <th>first loading time hours/min/sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GloVe common_crawl_840</td>\n",
       "      <td>2126</td>\n",
       "      <td>840 billion</td>\n",
       "      <td>2 196 016</td>\n",
       "      <td>cased</td>\n",
       "      <td>Common Crawl</td>\n",
       "      <td>Stanford Univ</td>\n",
       "      <td>18.329635</td>\n",
       "      <td>81.670365</td>\n",
       "      <td>15.604521</td>\n",
       "      <td>84.395479</td>\n",
       "      <td>1:46:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GloVe common_crawl_42</td>\n",
       "      <td>1833</td>\n",
       "      <td>42 billion</td>\n",
       "      <td>1 917 495</td>\n",
       "      <td>uncased</td>\n",
       "      <td>Common Crawl</td>\n",
       "      <td>Stanford Univ</td>\n",
       "      <td>14.806664</td>\n",
       "      <td>85.193336</td>\n",
       "      <td>12.110459</td>\n",
       "      <td>87.889541</td>\n",
       "      <td>1:30:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GloVe twitter</td>\n",
       "      <td>1484</td>\n",
       "      <td>27 billion</td>\n",
       "      <td>1 193 515</td>\n",
       "      <td>uncased</td>\n",
       "      <td>Tweets</td>\n",
       "      <td>Stanford Univ</td>\n",
       "      <td>42.868575</td>\n",
       "      <td>57.131425</td>\n",
       "      <td>41.387895</td>\n",
       "      <td>58.612105</td>\n",
       "      <td>00:27:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GloVe wikipedia_gigaword</td>\n",
       "      <td>841</td>\n",
       "      <td>6 billion</td>\n",
       "      <td>400 000</td>\n",
       "      <td>uncased</td>\n",
       "      <td>Wikipedia 2014 + Gigaword 5</td>\n",
       "      <td>Stanford Univ</td>\n",
       "      <td>22.313666</td>\n",
       "      <td>77.686334</td>\n",
       "      <td>19.901750</td>\n",
       "      <td>80.098250</td>\n",
       "      <td>00:03:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fasttext fasttext_wiki</td>\n",
       "      <td>10114</td>\n",
       "      <td>more than 100 millions</td>\n",
       "      <td>2 518 927</td>\n",
       "      <td>uncased</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>P. Bojanowski*, E. Grave, A. Joulin, T. Mikolov</td>\n",
       "      <td>20.498527</td>\n",
       "      <td>79.501473</td>\n",
       "      <td>19.332284</td>\n",
       "      <td>80.667716</td>\n",
       "      <td>12:10:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fasttext fasttext_wiki_news</td>\n",
       "      <td>665</td>\n",
       "      <td>16 billion</td>\n",
       "      <td>1 million</td>\n",
       "      <td>cased</td>\n",
       "      <td>Wikipedia 2017, UMBC webbase corpus and statmt...</td>\n",
       "      <td>T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsc...</td>\n",
       "      <td>19.321778</td>\n",
       "      <td>80.678222</td>\n",
       "      <td>17.040111</td>\n",
       "      <td>82.959889</td>\n",
       "      <td>00:29:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fasttext fasttext_wiki_news_subword</td>\n",
       "      <td>574</td>\n",
       "      <td>16 billion</td>\n",
       "      <td>1 million</td>\n",
       "      <td>cased</td>\n",
       "      <td>Wikipedia 2017, UMBC webbase corpus and statmt...</td>\n",
       "      <td>T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsc...</td>\n",
       "      <td>19.321778</td>\n",
       "      <td>80.678222</td>\n",
       "      <td>17.040111</td>\n",
       "      <td>82.959889</td>\n",
       "      <td>00:27:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fasttext crawl</td>\n",
       "      <td>1488</td>\n",
       "      <td>600 billion</td>\n",
       "      <td>2 million</td>\n",
       "      <td>cased</td>\n",
       "      <td>Common Crawl</td>\n",
       "      <td>T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsc...</td>\n",
       "      <td>18.175948</td>\n",
       "      <td>81.824052</td>\n",
       "      <td>15.667477</td>\n",
       "      <td>84.332523</td>\n",
       "      <td>1:43:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fasttext fasttext_crawl_subword</td>\n",
       "      <td>5691</td>\n",
       "      <td>600 billion</td>\n",
       "      <td>2 million</td>\n",
       "      <td>cased</td>\n",
       "      <td>Common Crawl</td>\n",
       "      <td>T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsc...</td>\n",
       "      <td>18.044087</td>\n",
       "      <td>81.073115</td>\n",
       "      <td>14.811211</td>\n",
       "      <td>84.464158</td>\n",
       "      <td>3:50:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  name size(Mb) number of words in training  \\\n",
       "0               GloVe common_crawl_840     2126                 840 billion   \n",
       "1                GloVe common_crawl_42     1833                  42 billion   \n",
       "2                        GloVe twitter     1484                  27 billion   \n",
       "3             GloVe wikipedia_gigaword      841                   6 billion   \n",
       "4               fasttext fasttext_wiki    10114      more than 100 millions   \n",
       "5          fasttext fasttext_wiki_news      665                  16 billion   \n",
       "6  fasttext fasttext_wiki_news_subword      574                  16 billion   \n",
       "7                       fasttext crawl     1488                 600 billion   \n",
       "8      fasttext fasttext_crawl_subword     5691                 600 billion   \n",
       "\n",
       "  number of unique tokens    cased  \\\n",
       "0               2 196 016    cased   \n",
       "1               1 917 495  uncased   \n",
       "2               1 193 515  uncased   \n",
       "3                 400 000  uncased   \n",
       "4               2 518 927  uncased   \n",
       "5               1 million    cased   \n",
       "6               1 million    cased   \n",
       "7               2 million    cased   \n",
       "8               2 million    cased   \n",
       "\n",
       "                                     source of words  \\\n",
       "0                                       Common Crawl   \n",
       "1                                       Common Crawl   \n",
       "2                                             Tweets   \n",
       "3                        Wikipedia 2014 + Gigaword 5   \n",
       "4                                          Wikipedia   \n",
       "5  Wikipedia 2017, UMBC webbase corpus and statmt...   \n",
       "6  Wikipedia 2017, UMBC webbase corpus and statmt...   \n",
       "7                                      Common Crawl    \n",
       "8                                      Common Crawl    \n",
       "\n",
       "                                             made by  missing words %  \\\n",
       "0                                      Stanford Univ        18.329635   \n",
       "1                                      Stanford Univ        14.806664   \n",
       "2                                      Stanford Univ        42.868575   \n",
       "3                                      Stanford Univ        22.313666   \n",
       "4    P. Bojanowski*, E. Grave, A. Joulin, T. Mikolov        20.498527   \n",
       "5  T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsc...        19.321778   \n",
       "6  T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsc...        19.321778   \n",
       "7  T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsc...        18.175948   \n",
       "8  T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsc...        18.044087   \n",
       "\n",
       "   embedding coverage %  missing words with latin letters %  \\\n",
       "0             81.670365                           15.604521   \n",
       "1             85.193336                           12.110459   \n",
       "2             57.131425                           41.387895   \n",
       "3             77.686334                           19.901750   \n",
       "4             79.501473                           19.332284   \n",
       "5             80.678222                           17.040111   \n",
       "6             80.678222                           17.040111   \n",
       "7             81.824052                           15.667477   \n",
       "8             81.073115                           14.811211   \n",
       "\n",
       "   embedding coverage with latin letters % first loading time hours/min/sec  \n",
       "0                                84.395479                          1:46:38  \n",
       "1                                87.889541                          1:30:10  \n",
       "2                                58.612105                         00:27:06  \n",
       "3                                80.098250                         00:03:58  \n",
       "4                                80.667716                         12:10:02  \n",
       "5                                82.959889                         00:29:42  \n",
       "6                                82.959889                         00:27:12  \n",
       "7                                84.332523                          1:43:49  \n",
       "8                                84.464158                          3:50:24  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>123</th>\n",
       "      <th>3</th>\n",
       "      <th>cyclotron</th>\n",
       "      <th>Knowles-Carter</th>\n",
       "      <th>New</th>\n",
       "      <th>York</th>\n",
       "      <th>New York</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fasttext_wiki</td>\n",
       "      <td>not in dict</td>\n",
       "      <td>not in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>not in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>not in dict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fasttext_wiki_news</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>not in dict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fasttext_wiki_news_subword</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>not in dict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fasttext_crawl</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>not in dict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fasttext_crawl_subword</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>in dict</td>\n",
       "      <td>not in dict</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name          123            3 cyclotron  \\\n",
       "0               fasttext_wiki  not in dict  not in dict   in dict   \n",
       "1          fasttext_wiki_news      in dict      in dict   in dict   \n",
       "2  fasttext_wiki_news_subword      in dict      in dict   in dict   \n",
       "3              fasttext_crawl      in dict      in dict   in dict   \n",
       "4      fasttext_crawl_subword      in dict      in dict   in dict   \n",
       "\n",
       "  Knowles-Carter      New     York     New York  \n",
       "0    not in dict  in dict  in dict  not in dict  \n",
       "1        in dict  in dict  in dict  not in dict  \n",
       "2        in dict  in dict  in dict  not in dict  \n",
       "3        in dict  in dict  in dict  not in dict  \n",
       "4        in dict  in dict  in dict  not in dict  "
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words = ['123', '3', 'cyclotron', 'Knowles-Carter', 'New', 'York', 'New York']\n",
    "test_words_lowercased = ['123', '3', 'cyclotron', 'knowles-carter', 'new', 'york', 'new york']\n",
    "fasttext_model_names = ['fasttext_wiki', 'fasttext_wiki_news', 'fasttext_wiki_news_subword', 'fasttext_crawl', \n",
    "                        'fasttext_crawl_subword']\n",
    "fasttext_model_case = ['uncased','cased','cased','cased', 'cased']\n",
    "fasttext_models = [fasttext_wiki, fasttext_wiki_news, fasttext_wiki_news_subword, fasttext_crawl, fasttext_crawl_subword]\n",
    "\n",
    "test_df = utils.process_test_words(fasttext_model_names, fasttext_model_case, test_words, test_words_lowercased, fasttext_models)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best pretrained model among fasttext models is fasttext_crawl_subword as it has the less percentage of uncovered SQuAD words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec is a two-layer neural net that processes text. Its input is a text corpus and its output is a set of vectors: feature vectors for words in that corpus. \n",
    "\n",
    "The output of the Word2vec neural net is a vocabulary in which each item has a vector attached to it, which can be fed into a deep-learning net or simply queried to detect relationships between words.\n",
    "\n",
    "It does so in one of two ways, either using context to predict a target word (a method known as continuous bag of words, or CBOW), or using a word to predict a target context, which is called skip-gram. \n",
    "\n",
    "https://skymind.ai/wiki/word2vec\n",
    "\n",
    "pretrained models: full list (https://developer.syn.co.in/tutorial/bot/oscova/pretrained-vectors.html)\n",
    "\n",
    "In the SKIPGRAM embedding algorithm, the contexts of a word w are the words surrounding it in the text - it is a linear bag-of-words algorithm for choosing the context. Using a window of size k around the target word w, 2k contexts are produced: the\n",
    "k words before and the k words after w. There is another algorithm for choosing contexts of a word in SKIPGRAM model: dependency-Based contexts - an alternative to the bag-of-words approach is to derive contexts based on the syntactic relations the word participates in.\n",
    "\n",
    "https://levyomer.files.wordpress.com/2014/04/dependency-based-word-embeddings-acl-2014.pdf\n",
    "\n",
    "|Model file\t| Number of dimensions|\tCorpus (size) |\tVocabulary size | Author |\tArchitecture |\tTraining Algorithm |\tContext window - size\t| Web page |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| Google News | 300 |\tGoogle News (100B) |\t3M |\tGoogle |\tword2vec |\tnegative sampling |\tBoW - ~5 | https://code.google.com/archive/p/word2vec/ |\n",
    "| Freebase IDs | 1000 |\tGooogle News (100B) |\t1.4M |\tGoogle |\tword2vec, skip-gram\t| ? |\tBoW - ~10 |\thttps://code.google.com/archive/p/word2vec/ | \n",
    "| Freebase names | 1000 |\tGooogle News (100B) |\t1.4M |\tGoogle |\tword2vec, skip-gram\t| ? |\tBoW - ~10 |\thttps://code.google.com/archive/p/word2vec/ |\n",
    "| Wikipedia dependency | 300 | Wikipedia (?) | 174,015 | Levy & Goldberg | word2vec modified | word2vec\t| syntactic dependencies | https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/ |\n",
    "| DBPedia vectors (wiki2vec) | 1000\t| Wikipedia (?)\t| ? | Idio\t| word2vec | word2vec, skip-gram | BoW, 10\t| https://github.com/idio/wiki2vec#prebuilt-models |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained model includes word vectors for a vocabulary of 3 million words and phrases that were trained on roughly 100 billion words from a Google News dataset. The vector length is 300 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Time: 275.86 sec.\n"
     ]
    }
   ],
   "source": [
    "#reload time (initial load time = 6:11:00)\n",
    "start_time = time.time()\n",
    "word2vec_google_news = Word_Emb(url='https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', \n",
    "               emb_alg = 'word2vec', short_name = 'google_news', binary = True, \n",
    "               bin_name = 'GoogleNews-vectors-negative300.bin')\n",
    "print(\"Done. Time: {} sec.\".format(round(time.time()-start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 109964/109964 [11:44<00:00, 156.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of word2vec word2vec_google_news : 39267, percentage of successful word embedding 64.29104070423048%, unsuccessful 35.708959295769525%\n",
      "Done. Time: 704.82 sec.\n"
     ]
    }
   ],
   "source": [
    "words_out_of_model_word2vec_google_news = utils.get_missing_words('word2vec word2vec_google_news', \n",
    "                                                              word2vec_google_news, utils.get_word_list(squad_words_and_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 109584/109584 [10:03<00:00, 181.53it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_df['word2vec word2vec_google_news'] = utils.get_word_existence_for_WE(word2vec_google_news,\n",
    "                                                                         utils.get_word_list(squad_words_and_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 104835/104835 [00:09<00:00, 11129.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of word2vec word2vec_google_news : 34925, percentage of successful word embedding 66.68574426479707%, unsuccessful 33.314255735202934%\n",
      "Done. Time: 9.42 sec.\n"
     ]
    }
   ],
   "source": [
    "eng_words_out_of_model_word2vec_google_news = utils.get_missing_words('word2vec word2vec_google_news', \n",
    "                                                      word2vec_google_news, utils.get_word_list(squad_words_and_freqs_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 104455/104455 [00:08<00:00, 12148.06it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_eng_df['word2vec word2vec_google_news'] = utils.get_word_existence_for_WE(word2vec_google_news,\n",
    "                                                                         utils.get_word_list(squad_words_and_freqs_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squad_words_cnt = len(utils.get_word_list(squad_words_and_freqs))\n",
    "squad_words_en_cnt = len(utils.get_word_list(squad_words_and_freqs_eng))\n",
    "results_df.loc[len(results_df)] = ['word2vec google_news', 1608, '100 billion', '3 million', \n",
    "                                    'cased', 'Google News', 'Google',\n",
    "                                    len(words_out_of_model_word2vec_google_news) / squad_words_cnt * 100,\n",
    "                                    100 -  len(words_out_of_model_word2vec_google_news) / squad_words_cnt * 100,\n",
    "                                    len(eng_words_out_of_model_word2vec_google_news) / squad_words_en_cnt * 100,\n",
    "                                    100 -  len(eng_words_out_of_model_word2vec_google_news) / squad_words_en_cnt * 100,\n",
    "                                    '6:11:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save results with word existence\n",
    "squad_words_and_freqs_df.to_excel(\"../data/squad/word_freqs.xlsx\")\n",
    "squad_words_and_freqs_eng_df.to_excel(\"../data/squad/word_freqs_eng.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freebase IDs, Freebase names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- freebase-vectors-skipgram1000.bin.gz (Freebase IDs): Entity vectors trained on 100B words from various news articles. Key value is Freebase identifier\n",
    "- freebase-vectors-skipgram1000-en.bin.gz (Freebase names): Entity vectors trained on 100B words from various news articles, using the deprecated /en/ naming (more easily readable); the vectors are sorted by frequency. the same vectors as Freebase IDs, but key value is /en/+ word\n",
    "\n",
    "\n",
    "The skipgram model with negative sampling is used to train the vectors in Google Freebase. The vectors in this dataset have 1000 dimensions in length. For preparing\n",
    "the embedding for phrases, they used a statistical approach to find words that appear more together than separately and then considered them as a single token.\n",
    "In the next step, they replaced these tokens with their corresponding freebase\n",
    "ID. Freebase is a knowledge base containing millions of entities and concepts,\n",
    "mostly extracted from Wikipedia pages.\n",
    "\n",
    "https://arxiv.org/pdf/1702.03470.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Time: 80.4 sec.\n"
     ]
    }
   ],
   "source": [
    "#reload time\n",
    "start_time = time.time()\n",
    "word2vec_freebase_ids = Word_Emb(url='https://drive.google.com/uc?export=download&confirm=u93n&id=0B7XkCwpI5KDYeFdmcVltWkhtbmM', \n",
    "               emb_alg = 'word2vec', short_name = 'freebase_ids', binary = True, bin_name = 'freebase_ids.bin')\n",
    "print(\"Done. Time: {} sec.\".format(round(time.time()-start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#squad words to freebase key format \n",
    "squad_words_uncased_en = ['/en/'+w for w in squad_words_uncased]\n",
    "eng_squad_words_uncased_en = ['/en/'+w for w in squad_words_eng_uncased]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 109964/109964 [01:35<00:00, 1147.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of word2vec word2vec_freebase_ids : 75696, percentage of successful word embedding 31.16292604852498%, unsuccessful 68.83707395147502%\n",
      "Done. Time: 95.85 sec.\n"
     ]
    }
   ],
   "source": [
    "words_out_of_model_word2vec_freebase_ids = utils.get_missing_words('word2vec word2vec_freebase_ids', \n",
    "                                                                          word2vec_freebase_ids, squad_words_uncased_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 104835/104835 [00:07<00:00, 13617.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of word2vec word2vec_freebase_ids : 70567, percentage of successful word embedding 32.68755663661945%, unsuccessful 67.31244336338055%\n",
      "Done. Time: 7.7 sec.\n"
     ]
    }
   ],
   "source": [
    "eng_words_out_of_model_word2vec_freebase_ids = utils.get_missing_words('word2vec word2vec_freebase_ids', \n",
    "                                                                          word2vec_freebase_ids, eng_squad_words_uncased_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows: 1422903\n",
      "[('/en/united_states', b'\\x00\\x00\\x80?\\x00\\x00\\x00@'), ('/en/associated_press', b'\\x00\\x00\\x80?\\x00\\x00\\x00@'), ('/en/barack_obama', b'\\x00\\x00\\x80?\\x00\\x00\\x00@'), ('/en/china', b'\\x00\\x00\\x80?\\x00\\x00\\x00@'), ('/en/united_kingdom', b'\\x00\\x00\\x80?\\x00\\x00\\x00@'), ('/en/new_york', b'\\x00\\x00\\x80?\\x00\\x00\\x00@'), ('/en/india', b'\\x00\\x00\\x80?\\x00\\x00\\x00@'), ('/en/europe', b'\\x00\\x00\\x80?\\x00\\x00\\x00@'), ('/en/washington_united_states', b'\\x00\\x00\\x80?\\x00\\x00\\x00@'), ('/en/canada', b'\\x00\\x00\\x80?\\x00\\x00\\x00@'), ('/en/iraq', b'\\x00\\x00\\x80?\\x00\\x00\\x00@'), ('/en/israel', b'\\x00\\x00\\x80?\\x00\\x00\\x00@'), ('/en/california', b'\\x00\\x00\\x80?\\x00\\x00\\x00@'), ('/en/republican_party', b'\\x00\\x00\\x80?\\x00\\x00\\x00@'), ('/en/austraila', b'\\x00\\x00\\x80?\\x00\\x00\\x00@')]\n"
     ]
    }
   ],
   "source": [
    "#select first 15 rows from database\n",
    "import sqlite3\n",
    " \n",
    "conn = sqlite3.connect(\"C:\\\\MRC\\\\squad\\\\.embeddings\\\\word2vec\\\\freebase_ids.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT max(rowid) from embeddings\")\n",
    "print('number of rows: %i' % cursor.fetchone()[0])\n",
    "cursor.execute(\"select * from embeddings limit 15\")\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df.loc[len(results_df)] = ['word2vec freebase_ids', 2473, '100 billion', '1.4 million', \n",
    "                                    'uncased', 'Google News', 'Google',\n",
    "                                    len(words_out_of_model_word2vec_freebase_ids) / len(squad_words) * 100,\n",
    "                                    100 -  len(words_out_of_model_word2vec_freebase_ids) / len(squad_words) * 100,\n",
    "                                    len(eng_words_out_of_model_word2vec_freebase_ids) / len(squad_words_eng) * 100,\n",
    "                                    100 -  len(eng_words_out_of_model_word2vec_freebase_ids) / len(squad_words_eng) * 100,\n",
    "                                    '00:23:59']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing word percentage is very high for word2vec_freebase_ids because this model use _ as a word separator and also it is the one model which doesn't divide namings into separate words (for example united_kingdom, new_york)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBpedia vectors (wiki2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Wikipedia page is represented by DBpedia resource.  We use prebuilt model for the English Wikipedia, without stemming, vector dimension is 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initial load time\n",
    "word2vec_dbpedia = Word_Emb(emb_alg = 'word2vec', short_name = 'dbpedia', shrink_vector_space = True, \n",
    "                            binary = False, bin_name = 'en.model', open_as_text = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 109964/109964 [01:20<00:00, 1369.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of word2vec word2vec_dbpedia : 29111, percentage of successful word embedding 73.52679058600997%, unsuccessful 26.473209413990034%\n",
      "Done. Time: 80.31 sec.\n"
     ]
    }
   ],
   "source": [
    "words_out_of_model_word2vec_dbpedia = utils.get_missing_words('word2vec word2vec_dbpedia',word2vec_dbpedia, \n",
    "                                                              utils.get_word_list(squad_words_and_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 109584/109584 [01:10<00:00, 1551.06it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_df['word2vec word2vec_dbpedia'] = utils.get_word_existence_for_WE(word2vec_dbpedia,\n",
    "                                                                         utils.get_word_list(squad_words_and_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 104835/104835 [00:08<00:00, 12709.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of word2vec word2vec_dbpedia : 25640, percentage of successful word embedding 75.54251919683313%, unsuccessful 24.45748080316688%\n",
      "Done. Time: 8.25 sec.\n"
     ]
    }
   ],
   "source": [
    "eng_words_out_of_model_word2vec_dbpedia = utils.get_missing_words('word2vec word2vec_dbpedia',word2vec_dbpedia, \n",
    "                                                                  utils.get_word_list(squad_words_and_freqs_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 104455/104455 [00:07<00:00, 13451.26it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_eng_df['word2vec word2vec_dbpedia'] = utils.get_word_existence_for_WE(word2vec_dbpedia,\n",
    "                                                                         utils.get_word_list(squad_words_and_freqs_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squad_words_cnt = len(utils.get_word_list(squad_words_and_freqs))\n",
    "squad_words_en_cnt = len(utils.get_word_list(squad_words_and_freqs_eng))\n",
    "results_df.loc[len(results_df)] = ['word2vec dbpedia', 4496, '100 billion', '1 151 090', \n",
    "                                    'cased', 'Wikipedia', 'Idio',\n",
    "                                    len(words_out_of_model_word2vec_dbpedia) / squad_words_cnt * 100,\n",
    "                                    100 -  len(words_out_of_model_word2vec_dbpedia) / squad_words_cnt * 100,\n",
    "                                    len(eng_words_out_of_model_word2vec_dbpedia) / squad_words_en_cnt * 100,\n",
    "                                    100 -  len(eng_words_out_of_model_word2vec_dbpedia) / squad_words_en_cnt * 100,\n",
    "                                    '00:15:49']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save results with word existence\n",
    "squad_words_and_freqs_df.to_excel(\"../data/squad/word_freqs.xlsx\")\n",
    "squad_words_and_freqs_eng_df.to_excel(\"../data/squad/word_freqs_eng.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia dependency was trained over context extracted from a dependency analysis of Wikipedia articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initial load time\n",
    "word2vec_wiki_dependency = Word_Emb(emb_alg = 'word2vec', short_name = 'wiki_dependency', shrink_vector_space = True, \n",
    "                            binary = False, vec_name = 'deps.words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 109964/109964 [00:08<00:00, 12857.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of word2vec word2vec_wiki_dependency : 34102, percentage of successful word embedding 68.98803244698266%, unsuccessful 31.01196755301735%\n",
      "Done. Time: 8.56 sec.\n"
     ]
    }
   ],
   "source": [
    "words_out_of_model_word2vec_wiki_dependency = utils.get_missing_words('word2vec word2vec_wiki_dependency',\n",
    "                                              word2vec_wiki_dependency, utils.get_word_list(squad_words_and_freqs_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 95465/95465 [00:06<00:00, 14144.25it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_uncased_df['word2vec word2vec_wiki_dependency'] = utils.get_word_existence_for_WE(word2vec_wiki_dependency,\n",
    "                                                                 utils.get_word_list(squad_words_and_freqs_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 104835/104835 [00:07<00:00, 13467.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words out of word2vec word2vec_wiki_dependency : 30049, percentage of successful word embedding 71.33686268898745%, unsuccessful 28.663137311012544%\n",
      "Done. Time: 7.79 sec.\n"
     ]
    }
   ],
   "source": [
    "eng_words_out_of_model_word2vec_wiki_dependency = utils.get_missing_words('word2vec word2vec_wiki_dependency',\n",
    "                                              word2vec_wiki_dependency, utils.get_word_list(squad_words_and_freqs_eng_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 90601/90601 [00:06<00:00, 13804.03it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_words_and_freqs_eng_uncased_df['word2vec word2vec_wiki_dependency'] = utils.get_word_existence_for_WE(word2vec_wiki_dependency,\n",
    "                            utils.get_word_list(squad_words_and_freqs_eng_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squad_words_cnt = len(utils.get_word_list(squad_words_and_freqs_uncased))\n",
    "squad_words_en_cnt = len(utils.get_word_list(squad_words_and_freqs_eng_uncased))\n",
    "results_df.loc[len(results_df)] = ['word2vec wiki_dependency', 839, '100 billion', '174 015', \n",
    "                                    'cased', 'Wikipedia', 'Levy & Goldberg',\n",
    "                                    len(words_out_of_model_word2vec_wiki_dependency) / len(squad_words) * 100,\n",
    "                                    100 -  len(words_out_of_model_word2vec_wiki_dependency) / len(squad_words) * 100,\n",
    "                                    len(eng_words_out_of_model_word2vec_wiki_dependency) / len(squad_words_eng) * 100,\n",
    "                                    100 -  len(eng_words_out_of_model_word2vec_wiki_dependency) / len(squad_words_eng) * 100,\n",
    "                                    '00:05:03']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save results with word existence\n",
    "squad_words_and_freqs_uncased_df.to_excel(\"../data/squad/word_freqs_uncased.xlsx\")\n",
    "squad_words_and_freqs_eng_uncased_df.to_excel(\"../data/squad/word_freqs_eng_uncased.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size(Mb)</th>\n",
       "      <th>number of words in training</th>\n",
       "      <th>number of unique tokens</th>\n",
       "      <th>cased</th>\n",
       "      <th>source of words</th>\n",
       "      <th>made by</th>\n",
       "      <th>missing words %</th>\n",
       "      <th>embedding coverage %</th>\n",
       "      <th>missing words with latin letters %</th>\n",
       "      <th>embedding coverage with latin letters %</th>\n",
       "      <th>first loading time hours/min/sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GloVe common_crawl_840</td>\n",
       "      <td>2126</td>\n",
       "      <td>840 billion</td>\n",
       "      <td>2 196 016</td>\n",
       "      <td>cased</td>\n",
       "      <td>Common Crawl</td>\n",
       "      <td>Stanford Univ</td>\n",
       "      <td>18.329635</td>\n",
       "      <td>81.670365</td>\n",
       "      <td>15.604521</td>\n",
       "      <td>84.395479</td>\n",
       "      <td>1:46:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GloVe common_crawl_42</td>\n",
       "      <td>1833</td>\n",
       "      <td>42 billion</td>\n",
       "      <td>1 917 495</td>\n",
       "      <td>uncased</td>\n",
       "      <td>Common Crawl</td>\n",
       "      <td>Stanford Univ</td>\n",
       "      <td>14.806664</td>\n",
       "      <td>85.193336</td>\n",
       "      <td>12.110459</td>\n",
       "      <td>87.889541</td>\n",
       "      <td>1:30:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GloVe twitter</td>\n",
       "      <td>1484</td>\n",
       "      <td>27 billion</td>\n",
       "      <td>1 193 515</td>\n",
       "      <td>uncased</td>\n",
       "      <td>Tweets</td>\n",
       "      <td>Stanford Univ</td>\n",
       "      <td>42.868575</td>\n",
       "      <td>57.131425</td>\n",
       "      <td>41.387895</td>\n",
       "      <td>58.612105</td>\n",
       "      <td>00:27:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GloVe wikipedia_gigaword</td>\n",
       "      <td>841</td>\n",
       "      <td>6 billion</td>\n",
       "      <td>400 000</td>\n",
       "      <td>uncased</td>\n",
       "      <td>Wikipedia 2014 + Gigaword 5</td>\n",
       "      <td>Stanford Univ</td>\n",
       "      <td>22.313666</td>\n",
       "      <td>77.686334</td>\n",
       "      <td>19.901750</td>\n",
       "      <td>80.098250</td>\n",
       "      <td>00:03:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fasttext fasttext_wiki</td>\n",
       "      <td>10114</td>\n",
       "      <td>more than 100 millions</td>\n",
       "      <td>2 518 927</td>\n",
       "      <td>uncased</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>P. Bojanowski*, E. Grave, A. Joulin, T. Mikolov</td>\n",
       "      <td>20.498527</td>\n",
       "      <td>79.501473</td>\n",
       "      <td>19.332284</td>\n",
       "      <td>80.667716</td>\n",
       "      <td>12:10:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fasttext fasttext_wiki_news</td>\n",
       "      <td>665</td>\n",
       "      <td>16 billion</td>\n",
       "      <td>1 million</td>\n",
       "      <td>cased</td>\n",
       "      <td>Wikipedia 2017, UMBC webbase corpus and statmt...</td>\n",
       "      <td>T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsc...</td>\n",
       "      <td>19.321778</td>\n",
       "      <td>80.678222</td>\n",
       "      <td>17.040111</td>\n",
       "      <td>82.959889</td>\n",
       "      <td>00:29:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fasttext fasttext_wiki_news_subword</td>\n",
       "      <td>574</td>\n",
       "      <td>16 billion</td>\n",
       "      <td>1 million</td>\n",
       "      <td>cased</td>\n",
       "      <td>Wikipedia 2017, UMBC webbase corpus and statmt...</td>\n",
       "      <td>T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsc...</td>\n",
       "      <td>19.321778</td>\n",
       "      <td>80.678222</td>\n",
       "      <td>17.040111</td>\n",
       "      <td>82.959889</td>\n",
       "      <td>00:27:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fasttext crawl</td>\n",
       "      <td>1488</td>\n",
       "      <td>600 billion</td>\n",
       "      <td>2 million</td>\n",
       "      <td>cased</td>\n",
       "      <td>Common Crawl</td>\n",
       "      <td>T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsc...</td>\n",
       "      <td>18.175948</td>\n",
       "      <td>81.824052</td>\n",
       "      <td>15.667477</td>\n",
       "      <td>84.332523</td>\n",
       "      <td>1:43:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fasttext fasttext_crawl_subword</td>\n",
       "      <td>5691</td>\n",
       "      <td>600 billion</td>\n",
       "      <td>2 million</td>\n",
       "      <td>cased</td>\n",
       "      <td>Common Crawl</td>\n",
       "      <td>T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsc...</td>\n",
       "      <td>18.044087</td>\n",
       "      <td>81.073115</td>\n",
       "      <td>14.811211</td>\n",
       "      <td>84.464158</td>\n",
       "      <td>3:50:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>word2vec google_news</td>\n",
       "      <td>1608</td>\n",
       "      <td>100 billion</td>\n",
       "      <td>3 million</td>\n",
       "      <td>cased</td>\n",
       "      <td>Google News</td>\n",
       "      <td>Google</td>\n",
       "      <td>35.708959</td>\n",
       "      <td>64.291041</td>\n",
       "      <td>33.314256</td>\n",
       "      <td>66.685744</td>\n",
       "      <td>6:11:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>word2vec freebase_ids</td>\n",
       "      <td>2473</td>\n",
       "      <td>100 billion</td>\n",
       "      <td>1.4 million</td>\n",
       "      <td>uncased</td>\n",
       "      <td>Google News</td>\n",
       "      <td>Google</td>\n",
       "      <td>68.837074</td>\n",
       "      <td>31.162926</td>\n",
       "      <td>67.312443</td>\n",
       "      <td>32.687557</td>\n",
       "      <td>00:23:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>word2vec dbpedia</td>\n",
       "      <td>4496</td>\n",
       "      <td>100 billion</td>\n",
       "      <td>1 151 090</td>\n",
       "      <td>cased</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>Idio</td>\n",
       "      <td>26.473209</td>\n",
       "      <td>73.526791</td>\n",
       "      <td>24.457481</td>\n",
       "      <td>75.542519</td>\n",
       "      <td>00:15:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>word2vec wiki_dependency</td>\n",
       "      <td>839</td>\n",
       "      <td>100 billion</td>\n",
       "      <td>174 015</td>\n",
       "      <td>cased</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>Levy &amp; Goldberg</td>\n",
       "      <td>31.011968</td>\n",
       "      <td>68.988032</td>\n",
       "      <td>28.663137</td>\n",
       "      <td>71.336863</td>\n",
       "      <td>00:05:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   name size(Mb) number of words in training  \\\n",
       "0                GloVe common_crawl_840     2126                 840 billion   \n",
       "1                 GloVe common_crawl_42     1833                  42 billion   \n",
       "2                         GloVe twitter     1484                  27 billion   \n",
       "3              GloVe wikipedia_gigaword      841                   6 billion   \n",
       "4                fasttext fasttext_wiki    10114      more than 100 millions   \n",
       "5           fasttext fasttext_wiki_news      665                  16 billion   \n",
       "6   fasttext fasttext_wiki_news_subword      574                  16 billion   \n",
       "7                        fasttext crawl     1488                 600 billion   \n",
       "8       fasttext fasttext_crawl_subword     5691                 600 billion   \n",
       "9                  word2vec google_news     1608                 100 billion   \n",
       "10                word2vec freebase_ids     2473                 100 billion   \n",
       "11                     word2vec dbpedia     4496                 100 billion   \n",
       "12             word2vec wiki_dependency      839                 100 billion   \n",
       "\n",
       "   number of unique tokens    cased  \\\n",
       "0                2 196 016    cased   \n",
       "1                1 917 495  uncased   \n",
       "2                1 193 515  uncased   \n",
       "3                  400 000  uncased   \n",
       "4                2 518 927  uncased   \n",
       "5                1 million    cased   \n",
       "6                1 million    cased   \n",
       "7                2 million    cased   \n",
       "8                2 million    cased   \n",
       "9                3 million    cased   \n",
       "10             1.4 million  uncased   \n",
       "11               1 151 090    cased   \n",
       "12                 174 015    cased   \n",
       "\n",
       "                                      source of words  \\\n",
       "0                                        Common Crawl   \n",
       "1                                        Common Crawl   \n",
       "2                                              Tweets   \n",
       "3                         Wikipedia 2014 + Gigaword 5   \n",
       "4                                           Wikipedia   \n",
       "5   Wikipedia 2017, UMBC webbase corpus and statmt...   \n",
       "6   Wikipedia 2017, UMBC webbase corpus and statmt...   \n",
       "7                                       Common Crawl    \n",
       "8                                       Common Crawl    \n",
       "9                                         Google News   \n",
       "10                                        Google News   \n",
       "11                                          Wikipedia   \n",
       "12                                          Wikipedia   \n",
       "\n",
       "                                              made by  missing words %  \\\n",
       "0                                       Stanford Univ        18.329635   \n",
       "1                                       Stanford Univ        14.806664   \n",
       "2                                       Stanford Univ        42.868575   \n",
       "3                                       Stanford Univ        22.313666   \n",
       "4     P. Bojanowski*, E. Grave, A. Joulin, T. Mikolov        20.498527   \n",
       "5   T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsc...        19.321778   \n",
       "6   T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsc...        19.321778   \n",
       "7   T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsc...        18.175948   \n",
       "8   T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsc...        18.044087   \n",
       "9                                              Google        35.708959   \n",
       "10                                             Google        68.837074   \n",
       "11                                               Idio        26.473209   \n",
       "12                                    Levy & Goldberg        31.011968   \n",
       "\n",
       "    embedding coverage %  missing words with latin letters %  \\\n",
       "0              81.670365                           15.604521   \n",
       "1              85.193336                           12.110459   \n",
       "2              57.131425                           41.387895   \n",
       "3              77.686334                           19.901750   \n",
       "4              79.501473                           19.332284   \n",
       "5              80.678222                           17.040111   \n",
       "6              80.678222                           17.040111   \n",
       "7              81.824052                           15.667477   \n",
       "8              81.073115                           14.811211   \n",
       "9              64.291041                           33.314256   \n",
       "10             31.162926                           67.312443   \n",
       "11             73.526791                           24.457481   \n",
       "12             68.988032                           28.663137   \n",
       "\n",
       "    embedding coverage with latin letters % first loading time hours/min/sec  \n",
       "0                                 84.395479                          1:46:38  \n",
       "1                                 87.889541                          1:30:10  \n",
       "2                                 58.612105                         00:27:06  \n",
       "3                                 80.098250                         00:03:58  \n",
       "4                                 80.667716                         12:10:02  \n",
       "5                                 82.959889                         00:29:42  \n",
       "6                                 82.959889                         00:27:12  \n",
       "7                                 84.332523                          1:43:49  \n",
       "8                                 84.464158                          3:50:24  \n",
       "9                                 66.685744                          6:11:00  \n",
       "10                                32.687557                         00:23:59  \n",
       "11                                75.542519                         00:15:49  \n",
       "12                                71.336863                         00:05:03  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best pretrained model among word2vec models is dbpedia as it has the less percentage of uncovered SQuAD words. \n",
    "The best pretrained model among all models is GloVe common_crawl_42 as it has the less percentage of uncovered SQuAD words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMo is a deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). These word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. \n",
    "\n",
    "The representations are generated from a function of the entire sentence to create word-level representations. The embeddings are generated at a character-level, so they can capitalize on sub-word units like FastText and do not suffer from the issue of out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from embeddings.elmo import ElmoEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "elmo = ElmoEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding for canada = [-0.12928208708763123, 0.1797446757555008, 0.32043027877807617, 0.09774105250835419, -0.23000513017177582, 0.045169465243816376, 0.3189358711242676, 0.08198004961013794, 0.06236783415079117, -0.1548035591840744, 0.026339039206504822, 0.2822563052177429, 0.3813740015029907, -0.39079368114471436, 0.05570315569639206, -0.24463962018489838, -0.15392930805683136, -0.13432516157627106, 0.10219113528728485, 0.20746605098247528, -0.44593366980552673, -0.3348473012447357, -0.3258061707019806, 0.41351082921028137, 0.49114280939102173, -0.08096278458833694, 0.03477930277585983, -0.01950651779770851, 0.14569328725337982, 0.2253284752368927, -0.03725171834230423, 0.1801299750804901, 0.05353356897830963, -0.016776323318481445, 0.012518258765339851, 0.06441564857959747, -0.0976974368095398, 0.24552100896835327, -0.26171183586120605, -0.1044323593378067, -0.05325465649366379, 0.11762401461601257, 0.048691753298044205, 0.08610888570547104, -0.2276361882686615, 0.39680972695350647, 0.00115145742893219, -0.15537481009960175, 0.05891218036413193, 0.14884436130523682, 0.165128692984581, 0.3041325509548187, 0.2610974907875061, 0.16551603376865387, -0.17023906111717224, 0.1654636561870575, -0.4348123073577881, 0.07153503596782684, 0.35729360580444336, -0.1955939680337906, 0.3287225663661957, 0.4170553684234619, 0.0023902710527181625, 0.0042257159948349, -0.04870164394378662, 0.28121888637542725, -0.0017273128032684326, 0.34240055084228516, 0.10247146338224411, 0.04628666117787361, 0.07203780114650726, 0.12220735847949982, 0.2624342143535614, -0.3589985966682434, 0.2558884024620056, -0.023560181260108948, 0.20240749418735504, 0.18968132138252258, 0.2546845078468323, 0.06943853199481964, 0.13134443759918213, 0.06782425194978714, 0.49693500995635986, -0.27477893233299255, 0.2513478398323059, -0.09413556754589081, -0.24451598525047302, 0.11863747984170914, 0.0393395870923996, -0.053459495306015015, 0.19044646620750427, 0.09503983706235886, 0.4425569474697113, -0.24111808836460114, -0.22841671109199524, -0.0871700793504715, 0.07391588389873505, -0.12100106477737427, 0.06696818768978119, 0.10943350940942764, -0.21867838501930237, 0.03542222082614899, 0.2576376497745514, -0.40977582335472107, -0.16251063346862793, 0.5321401357650757, -0.31646788120269775, 5.169585347175598e-05, 0.2757939398288727, 0.19384533166885376, 0.36824315786361694, -0.23517751693725586, -0.39269426465034485, 0.1134226843714714, 0.24100284278392792, -0.27783888578414917, -0.0027200132608413696, 0.026462849229574203, 0.15124908089637756, 0.03836819529533386, -0.17262853682041168, 0.15289093554019928, -0.2868196666240692, -0.2094908058643341, 0.06499995291233063, -0.006108369678258896, -0.006438655778765678, -0.09895871579647064, -0.05184166505932808, -0.4100640118122101, -0.19519616663455963, -0.40423813462257385, -0.07989998161792755, -0.13974344730377197, 0.1253698319196701, -0.053449422121047974, -0.3103066086769104, -0.2772810757160187, -0.37172794342041016, -0.014604896306991577, -0.060510773211717606, -0.21803514659404755, 0.40207934379577637, -0.3134031295776367, 0.10611429810523987, 0.22943712770938873, 0.06380698829889297, -0.080670565366745, 0.1476949155330658, 0.07156408578157425, 0.16862276196479797, 0.16848382353782654, 0.04832863807678223, 0.5232518315315247, -0.22869449853897095, -0.1854039877653122, -0.011890105903148651, -0.03548584505915642, 0.1283521056175232, 0.1274527609348297, -0.11477579176425934, 0.1129782497882843, 0.08103150129318237, -0.39386582374572754, -0.22536762058734894, 0.044293541461229324, -0.07136430591344833, -0.10377448052167892, -0.1999538242816925, 0.2714064121246338, 0.12256337702274323, -0.06274662911891937, 0.052613306790590286, -0.11301375180482864, -0.19302998483181, -0.07649965584278107, -0.02068997733294964, -0.35439640283584595, -0.1675327718257904, 0.06575071066617966, -0.19969028234481812, 0.05482638627290726, 0.22524482011795044, -0.0579141266644001, 0.29569387435913086, -0.10554184764623642, -0.16343003511428833, -0.07100802659988403, -0.4027826189994812, 0.012217745184898376, -0.09944097697734833, -0.05399451404809952, 0.4275808036327362, 0.11094704270362854, -0.006863817572593689, 0.014395467936992645, -0.4369383752346039, 0.06596414744853973, -0.04134603589773178, 0.15932604670524597, 0.4327380359172821, -0.2187841236591339, 0.3098762035369873, 0.15162526071071625, -0.009151820093393326, 0.01756301522254944, 0.20419496297836304, 0.002694820985198021, -0.24310517311096191, 0.22899910807609558, -0.09749871492385864, 0.3333933353424072, -0.020369544625282288, 0.10195571929216385, -0.1853008270263672, 0.09805013984441757, 0.11255153268575668, -0.18429957330226898, -0.06028662621974945, -0.35856565833091736, 0.04390580952167511, 0.13033384084701538, -0.13173900544643402, -0.09457459300756454, -0.14416161179542542, 0.09375688433647156, 0.09828419983386993, 0.16272751986980438, -0.34195733070373535, 4.994496703147888e-05, -0.20996211469173431, 0.181757390499115, -0.07452771067619324, -0.04857318475842476, -0.04734746366739273, 0.07923580706119537, 0.08939988911151886, 0.25128060579299927, 0.12906278669834137, -0.3149647116661072, 0.014027301222085953, 0.26832014322280884, -0.0820583701133728, -0.16293787956237793, 0.18366160988807678, -0.3581199049949646, -0.19333365559577942, 0.49469178915023804, -0.16192743182182312, -0.14397171139717102, 0.05845467373728752, -0.034623607993125916, 0.14352375268936157, -0.07648350298404694, -0.11812116950750351, -0.21425551176071167, 0.25177037715911865, -0.09773611277341843, -0.3263162672519684, -0.1785426139831543, 0.41540488600730896, -0.07707589119672775, -0.14924874901771545, -0.13490451872348785, 0.059390611946582794, -0.2011984884738922, -0.14210912585258484, 0.16609066724777222, -0.10930993407964706, -0.08303489536046982, -0.2850712239742279, -0.26429927349090576, 0.0025193169713020325, -0.05637572705745697, -0.29949620366096497, 0.045108333230018616, -0.11435791850090027, 0.1633218377828598, -0.08354011923074722, 0.24671591818332672, -0.47786372900009155, 0.051434777677059174, 0.10276231169700623, -0.19038158655166626, 0.05882330238819122, 0.09021219611167908, 0.3053887188434601, -0.08749684691429138, -0.06340493261814117, 0.047016240656375885, 0.3015548586845398, -0.5798600912094116, 0.10823900997638702, -0.5929803848266602, -0.04738757014274597, -0.1440521776676178, -0.03810594975948334, 0.1470908224582672, 0.1056937426328659, 0.04123903065919876, 0.3339754343032837, -0.3657802641391754, 0.47279590368270874, -0.05424570292234421, -0.0966005027294159, -0.2413633167743683, -0.15327569842338562, -0.19808167219161987, -0.377643883228302, -0.168671652674675, -0.026687603443861008, 0.3554432690143585, 0.11169226467609406, -0.17603127658367157, -0.09218669682741165, -0.4458892345428467, 0.11415457725524902, 0.44500473141670227, 0.04909321665763855, 0.24059616029262543, 0.020751260221004486, -0.027822323143482208, -0.36714088916778564, -0.043872393667697906, -0.10293196141719818, -0.034800007939338684, 0.011233799159526825, -0.3465745449066162, -0.0997200459241867, -0.13786040246486664, 0.27877333760261536, -0.2208070456981659, -0.04055332764983177, 0.12180713564157486, -0.09515609592199326, -0.13123326003551483, 0.25668054819107056, 0.030046142637729645, 0.27705228328704834, -0.326331228017807, -0.19196952879428864, 0.07147606462240219, 0.18504276871681213, -0.13884010910987854, -0.23438292741775513, -0.15514056384563446, 0.41332757472991943, -0.11913912743330002, 0.058964163064956665, 0.23437228798866272, -0.308692067861557, 0.3140513598918915, 0.1739753633737564, -0.2628108859062195, 0.2547229826450348, 0.04235566407442093, -0.17419998347759247, -0.42686209082603455, 0.06726425886154175, -0.24368396401405334, 0.2547455430030823, 0.12839843332767487, 0.10872913151979446, -0.25843292474746704, 0.5543355345726013, 0.05426493659615517, 0.07876630127429962, -0.07222790271043777, -0.13963276147842407, -0.17077285051345825, 0.02685670554637909, -0.06795696914196014, 0.026939231902360916, 0.19611287117004395, -0.08603420853614807, -0.21266159415245056, 0.12268561124801636, -0.4171467423439026, 0.12359680980443954, 0.3728070855140686, 0.514914333820343, 0.5407861471176147, -0.22320182621479034, 0.043852224946022034, 0.05962219834327698, -0.417341023683548, -0.07184167951345444, 0.26174646615982056, -0.5819756984710693, -0.12702739238739014, -0.07679840177297592, 0.20007769763469696, -0.0637524276971817, 0.30765992403030396, -0.13955986499786377, -0.40955114364624023, 0.09162557125091553, 0.14612753689289093, -0.038044512271881104, 0.024697616696357727, 0.21593567728996277, 0.10275649279356003, 0.034264855086803436, -0.11443191766738892, 0.40727782249450684, -0.04277319088578224, -0.2825445532798767, 0.20170366764068604, -0.053324416279792786, -0.0181497260928154, -0.19577808678150177, -0.049128249287605286, -0.2637290060520172, -0.12258332222700119, -0.15426801145076752, 0.3699895441532135, -0.366380900144577, 0.24206586182117462, 0.23317837715148926, -0.12326496839523315, -0.01104259304702282, -0.10526198148727417, 0.19136714935302734, 0.1208009272813797, 0.1231595054268837, 0.2312779426574707, -0.15767940878868103, 0.34505027532577515, 0.1993226706981659, -0.4420124292373657, -0.26528510451316833, -0.04178251326084137, -0.17600315809249878, 0.1538802683353424, -0.38695257902145386, 0.07849002629518509, -0.10927842557430267, 0.18451900780200958, 0.41871458292007446, -0.2080552577972412, -0.3757607638835907, -0.2380661964416504, 0.13280907273292542, 0.017240632325410843, 0.01597246527671814, 0.18084409832954407, -0.28068894147872925, -0.14548049867153168, -0.12043681740760803, -0.2898341417312622, -0.1888618767261505, 0.1703331172466278, -0.101657435297966, 0.2988143861293793, 0.2613268494606018, -0.16480278968811035, 0.49387887120246887, 0.11223326623439789, -0.16223999857902527, -0.18264788389205933, 0.30391985177993774, -0.597818911075592, 0.1058160662651062, -0.21874231100082397, -0.13205048441886902, -0.1675085425376892, -0.20153920352458954, 0.36551007628440857, -0.1948794573545456, -0.049666546285152435, -0.019751736894249916, 0.03835318982601166, 0.10710377991199493, -0.09664429724216461, 0.048705197870731354, -0.07651828974485397, 0.10290870815515518, 0.05040860176086426, -0.03397804871201515, 0.3635528087615967, -0.1279616355895996, -0.3202958106994629, -0.06490596383810043, -0.10167178511619568, 0.0349913015961647, 0.0576961524784565, -0.09654408693313599, 0.13911283016204834, 0.09495768696069717, -0.20616647601127625, -0.09229893982410431, 0.5121829509735107, -0.05005105212330818, 0.044713251292705536, 0.13725462555885315, -0.06958352029323578, 0.18519937992095947, -0.016052797436714172, 0.06991815567016602, 0.24171948432922363, -0.02546747773885727, 0.15564988553524017, 0.1664009392261505, -0.11870382726192474, 0.26663240790367126, 0.11344726383686066, -0.021844178438186646, -0.07766881585121155, 0.31730249524116516, 0.07351648062467575, -0.11461102962493896, -0.010300714522600174]\n",
      "embedding for ghjffg = [-0.053191863000392914, 0.2111680656671524, 0.07583154737949371, 0.22642117738723755, -0.2571137845516205, -0.29173827171325684, -0.3422476351261139, 0.035625576972961426, -0.1039857417345047, -0.011952176690101624, -0.40939968824386597, 0.14555849134922028, -0.14899076521396637, -0.021205272525548935, -0.112340547144413, 0.0590304397046566, 0.35525327920913696, -0.08561190962791443, 0.11486052721738815, -0.3730452060699463, -0.29695746302604675, 0.15182220935821533, 0.08501408994197845, 0.211920365691185, -0.1183176189661026, 0.7390556931495667, 0.33752554655075073, -0.09424567967653275, -0.2819412052631378, 0.30825963616371155, -0.3955248296260834, -0.03363551199436188, -0.2811015248298645, -0.6450842618942261, -0.13184455037117004, -0.42014169692993164, -0.1227995902299881, -0.1513093113899231, 0.32768237590789795, -0.12552574276924133, 0.2667921781539917, 0.27619674801826477, -0.11268195509910583, 0.3105822205543518, -0.18090549111366272, 0.575136125087738, 0.3890521228313446, 0.14321766793727875, 0.040554434061050415, 0.650841474533081, -0.08805697411298752, 0.11612405627965927, 0.21711663901805878, -0.04085046425461769, 0.21190312504768372, 0.15944400429725647, -0.6250873804092407, 0.7628809809684753, -0.3906186521053314, 0.02223081886768341, -0.4045700132846832, -0.008278310298919678, -0.16522735357284546, 0.44414466619491577, -0.05278310179710388, -0.3036370873451233, -0.22169317305088043, 0.27001750469207764, -0.134558767080307, 0.05955797806382179, -0.03628271818161011, -0.1674860119819641, 0.3694191575050354, 0.7637180685997009, 0.20400723814964294, 0.050234973430633545, -0.3192230463027954, 0.16046583652496338, 0.3581441044807434, -0.2320900708436966, -0.46271204948425293, 0.3907665014266968, -0.014907333999872208, 0.28254279494285583, -0.25686195492744446, 0.05226917564868927, 0.21869906783103943, 0.47750917077064514, -0.022176280617713928, 0.07797843962907791, 0.33561643958091736, -0.11715169996023178, 0.17724919319152832, 0.19141578674316406, 0.2628260850906372, 0.2340277135372162, 0.31551676988601685, -0.25869977474212646, 0.010038398206233978, 0.298261821269989, 0.12083956599235535, -0.1721295416355133, -0.6042832732200623, -0.5378262996673584, -0.31723713874816895, 0.09010528028011322, 0.7115216851234436, -0.04581115394830704, 0.2189681977033615, 0.1815604567527771, -0.10308943688869476, -0.20318743586540222, 0.0976383239030838, -0.13491308689117432, 0.12706337869167328, 0.07175834476947784, 0.24247971177101135, -0.0020504742860794067, 0.32410383224487305, -0.5837506055831909, -0.7709551453590393, 0.3348914086818695, -0.16152054071426392, 0.1430976688861847, 0.6525614261627197, -0.1337735503911972, -0.14166571199893951, -0.6488386988639832, -0.2103913575410843, -0.2654220461845398, 0.14906693994998932, 0.331425279378891, 0.10860341787338257, 0.13762569427490234, 0.2579742968082428, 0.31403791904449463, -0.2452625036239624, -0.03388170897960663, 0.16867917776107788, 0.30830979347229004, -0.11857419461011887, -0.35180217027664185, -0.2712894082069397, -0.017296209931373596, -0.3344431221485138, 0.13265591859817505, -0.15363556146621704, -0.5287200212478638, 0.00043099746108055115, 0.02208259142935276, 0.12289828062057495, 0.5256057977676392, 0.36247289180755615, 0.5695083737373352, 0.37046200037002563, -0.27730321884155273, 0.30225756764411926, -0.7861891984939575, 0.23040010035037994, 0.06162828207015991, -0.5035334825515747, -0.0955667793750763, -0.07544800639152527, -0.21166370809078217, -0.13897480070590973, -0.0364501029253006, 0.5708949565887451, -0.3040734827518463, 0.1891043335199356, 0.23237207531929016, 0.29061591625213623, 0.1094508096575737, -0.08781889081001282, 0.10415952652692795, 0.2689902186393738, -0.26255354285240173, 0.09340079128742218, -0.005409136414527893, -0.36582550406455994, -0.1476939618587494, -0.011817805469036102, 0.06383033841848373, 0.2217087596654892, -0.21002736687660217, -0.04849579930305481, -0.006420105695724487, 0.1653539091348648, 0.17171776294708252, 0.07884690165519714, 0.629409909248352, -0.19081981480121613, 0.010116196237504482, 0.21969321370124817, 0.12614333629608154, 0.4560939073562622, 0.400261253118515, 0.09451186656951904, 0.19451268017292023, 0.23314440250396729, 0.05581548810005188, 0.17604243755340576, 0.5119433403015137, 0.6970943212509155, -0.4510667026042938, -0.06807856261730194, -0.05471387505531311, 0.2784573435783386, 0.19880834221839905, -0.44125351309776306, -0.011465340852737427, -0.20128241181373596, 0.6282963752746582, 0.14846159517765045, -0.1755731701850891, -0.03548338636755943, 0.21868276596069336, 0.49051398038864136, -0.08689123392105103, 0.1106560081243515, -0.06695859134197235, -0.17630605399608612, -0.13865438103675842, 0.05324925482273102, -0.29897424578666687, 0.15033376216888428, 0.00976107269525528, 0.41859662532806396, -0.510939359664917, -0.13918468356132507, -0.24903897941112518, -0.2361685037612915, 0.1051432266831398, -0.23266103863716125, 0.36189109086990356, -0.3058668375015259, 0.3305683434009552, 0.003029242157936096, 0.39281395077705383, -0.06972448527812958, -0.32224535942077637, -0.5946881175041199, 0.01961333304643631, -0.33023229241371155, -0.05386582762002945, 0.048251524567604065, -0.0448082871735096, 0.15164600312709808, 0.012159354984760284, -0.4465802311897278, -0.2199714332818985, 0.19790703058242798, -0.24862492084503174, 0.018324796110391617, 0.382266104221344, 0.12705381214618683, -0.07588405907154083, 0.3697388768196106, 0.28038063645362854, 0.2609039843082428, 0.27185025811195374, 0.5272210240364075, -0.6376851797103882, 0.06765338033437729, 0.0076331645250320435, 0.048385437577962875, 0.013611413538455963, -0.3625880479812622, -0.06143372133374214, 1.013857364654541, -0.28020942211151123, -0.3191242516040802, -0.06750469654798508, 0.4609123766422272, -0.2673322558403015, -0.10489742457866669, 0.10391844809055328, -0.1147368773818016, 0.012600989080965519, -0.22869092226028442, -0.1418764889240265, 0.36670103669166565, -0.2559756934642792, 0.06062142923474312, 0.013192053884267807, 0.023404687643051147, 0.5004702210426331, -0.33741894364356995, 0.06707414984703064, 0.10764531791210175, -0.4556846022605896, -0.008795693516731262, -0.550752580165863, -0.08599337935447693, 0.011854354292154312, -0.4766993224620819, -0.04567152261734009, -0.2753863036632538, -0.2603292465209961, -0.031992435455322266, 0.12783801555633545, 0.08240063488483429, 0.14517182111740112, 0.6715745329856873, 0.3267548978328705, 0.1555723249912262, -0.7400355935096741, -0.22550567984580994, -0.2881441116333008, -0.3919786810874939, -0.22992317378520966, -0.00480058416724205, 0.6345049738883972, -0.09419115632772446, 0.35758474469184875, 0.3070681691169739, 0.015756666660308838, 0.25390174984931946, -0.02716802805662155, 0.027039766311645508, 0.10374610126018524, 0.6466100215911865, -0.15838724374771118, -0.27676305174827576, -0.43982216715812683, 0.048638731241226196, 0.09680110961198807, 0.06759283691644669, -0.18415950238704681, -0.5362496972084045, 0.04900611937046051, -0.29023146629333496, -0.8381758332252502, 0.14835688471794128, 0.039679862558841705, 0.17293286323547363, 0.02785213477909565, 0.08746486902236938, 0.6616962552070618, 0.30200088024139404, 0.09659692645072937, -0.3113955855369568, -0.3968532979488373, -0.5122776031494141, -0.42636212706565857, -0.08992110937833786, 0.0501423254609108, -0.24361282587051392, -0.03021632879972458, 0.17304039001464844, -0.3427201509475708, -0.3359088599681854, -0.3460712432861328, 0.22363586723804474, -0.17146672308444977, 0.023856271058321, 0.04322054982185364, -0.08616052567958832, 0.27927908301353455, -0.14444266259670258, -0.18708929419517517, -0.08807123452425003, -0.36620643734931946, 0.20855900645256042, 0.1060897707939148, 0.6323841214179993, 0.18118751049041748, -0.17708241939544678, -0.6220948100090027, 0.054370202124118805, 0.3562867343425751, -0.033231332898139954, 0.39527571201324463, -0.11806463450193405, -0.2999245524406433, -0.0057883430272340775, 0.16407465934753418, 0.14097145199775696, 0.19545650482177734, 0.0330645814538002, 0.09083482623100281, 0.07697492837905884, 0.40607595443725586, 0.48404181003570557, -0.029408082365989685, 0.23412495851516724, -0.6919860243797302, -0.21731328964233398, 0.10293465852737427, 0.08889895677566528, -0.24651700258255005, -0.10366887599229813, 0.25602245330810547, -0.2688435912132263, 0.4010053873062134, -0.19755281507968903, -0.09126590192317963, 0.05367137864232063, -0.19229000806808472, 0.3091707229614258, -0.21556901931762695, 0.21897810697555542, -0.16947412490844727, 0.08684802055358887, 0.03844636678695679, -0.11272137612104416, 0.6146988868713379, 0.2742742598056793, 0.20569875836372375, -0.0774257481098175, 0.0469486340880394, -0.13265778124332428, -0.5383431911468506, -0.008858196437358856, -0.6342180371284485, -0.22010843455791473, 0.3035489618778229, -0.2767367959022522, -0.055580154061317444, -0.42112118005752563, -0.44865620136260986, 0.5190736055374146, -0.45885157585144043, 0.13226617872714996, 0.014780864119529724, 0.04992695897817612, 0.19344358146190643, 0.09039962291717529, 0.26431170105934143, 0.43350034952163696, 0.07040698826313019, -0.25079718232154846, 0.3094189465045929, 0.669266402721405, -0.18347415328025818, -0.572952389717102, 0.03449352830648422, -0.25137853622436523, -0.09597815573215485, 0.2794821560382843, 0.3184199333190918, -0.31263044476509094, 0.28803786635398865, 0.09833347797393799, -0.07927921414375305, -0.18047286570072174, 0.2626631259918213, -0.43495747447013855, -0.007546279579401016, -0.39912405610084534, -0.3982691168785095, -0.47295016050338745, -0.12914133071899414, 0.3685285449028015, 0.2887635827064514, 0.31899547576904297, -0.1298207938671112, -0.0017246417701244354, 0.41414952278137207, 0.4803808033466339, 0.00956319272518158, -0.04806283116340637, -0.5984430909156799, -0.5136564373970032, 0.23991388082504272, -0.6015452146530151, 0.12355675548315048, 0.2775091826915741, 0.14711956679821014, 0.14467982947826385, 0.14888015389442444, 0.38228026032447815, -0.21102449297904968, 0.13628870248794556, -0.14080660045146942, 0.014092518016695976, 0.5344211459159851, 0.2739889621734619, -0.33374977111816406, 0.6628022789955139, -0.1738554835319519, 0.30538684129714966, 0.04370979219675064, -0.003001225646585226, -0.4204021990299225, 0.3962487578392029, -0.17505361139774323, 0.17833784222602844, 0.367997407913208, -0.24915245175361633, 0.17377309501171112, 0.22502602636814117, 0.07243958860635757, -0.4078037738800049, -0.08104606717824936, -0.5030077695846558, -0.4524678885936737, -0.42183613777160645, -0.03943590819835663, -0.33978140354156494, -0.0876956358551979, 0.065116286277771, -0.006130736321210861, -0.3537803590297699, -0.33025363087654114, -0.08183653652667999, -0.4441704750061035, -0.1116890236735344, 0.01845371723175049, -0.0027159377932548523, 0.5429777503013611, 0.26296114921569824, -0.40465256571769714]\n",
      "embedding for cyclotron = [-0.12050152570009232, 0.05671950429677963, 0.2772097587585449, -0.28519272804260254, -0.5063488483428955, -0.7416640520095825, 0.4365825057029724, 0.07865308225154877, 0.27907851338386536, 0.017403244972229004, -0.513521671295166, 0.5072678327560425, 0.29213884472846985, -0.13612283766269684, -0.26999104022979736, 0.05518217757344246, 0.17633546888828278, 0.2198045700788498, 0.4110274314880371, -0.37477171421051025, 0.657818615436554, -0.012942858040332794, -0.12898331880569458, 0.9079463481903076, -0.017112065106630325, 0.6335442662239075, 0.015694081783294678, -0.04057596996426582, 0.29622045159339905, 0.07988891750574112, -0.06849632412195206, 0.16264593601226807, 0.2591882348060608, 0.057812221348285675, 0.5378804206848145, 0.4606272876262665, -0.521468997001648, 0.4895210862159729, -0.33901357650756836, -0.054569147527217865, -0.37190085649490356, 0.4077408015727997, -0.5860949158668518, -0.12092600762844086, 0.48702576756477356, 0.38722938299179077, 0.27083703875541687, 0.5370299220085144, -0.270943820476532, -0.34933799505233765, 0.1800130307674408, 0.024653218686580658, 0.49410608410835266, 0.1234930008649826, -0.36357393860816956, -0.21167755126953125, -0.5625773668289185, 0.7448579668998718, 0.03991588205099106, -0.5905143022537231, 0.2510732412338257, -0.09072152525186539, 0.11106336861848831, -0.3070470094680786, 0.5565990805625916, -0.058426037430763245, -0.4404815435409546, 0.10274538397789001, 0.030728423967957497, 0.03500084951519966, -0.2050156444311142, 0.0752401053905487, 0.2356497049331665, 0.2389170378446579, 0.5929849743843079, 0.01839768886566162, 0.04636530578136444, 0.9923968315124512, 0.48208093643188477, -0.18542851507663727, -0.17988896369934082, 0.08238474279642105, -0.208241268992424, 0.10540273040533066, 0.23949109017848969, -0.5159711837768555, 0.18706649541854858, -0.13483311235904694, 0.1074053943157196, 0.2737092077732086, 0.08449885994195938, 0.26475873589515686, 0.15474268794059753, -0.559083104133606, -0.11928882449865341, -0.4145949184894562, -0.44728174805641174, 0.26629388332366943, 0.44378021359443665, 0.2807465195655823, -0.03326735645532608, 0.10267077386379242, -0.5633852481842041, 0.30518287420272827, -0.662213921546936, -0.2996257543563843, -0.4488986134529114, 0.177090585231781, 0.2407676875591278, 0.13596990704536438, 0.36616194248199463, -0.2959434688091278, 0.10034745186567307, -0.2746126055717468, -0.005893155932426453, 0.016501426696777344, -0.16215044260025024, -0.29257917404174805, -0.23357251286506653, -0.533714771270752, -0.3843027651309967, 0.37503495812416077, 0.13843148946762085, -0.17578096687793732, -0.02492491528391838, -0.47009754180908203, -0.24627447128295898, 0.024481937289237976, -0.23346471786499023, -0.27615436911582947, -0.23673634231090546, -0.045878827571868896, -0.2850695550441742, 0.4083492159843445, -0.16488975286483765, 0.4107999801635742, 0.13218984007835388, -0.4213343858718872, -0.10055781900882721, 0.1848193109035492, 0.4933617115020752, -0.3016209006309509, -0.31103065609931946, 0.5419596433639526, -0.9043440818786621, -0.3040999472141266, 0.5167297124862671, -0.6513923406600952, -0.2957886755466461, 0.048431240022182465, 0.12429952621459961, -0.22991350293159485, -0.030761972069740295, 0.21327322721481323, -0.05317971110343933, -0.60538250207901, 0.5983192324638367, -0.38506945967674255, 0.46510931849479675, 0.754673957824707, 0.1707545965909958, 0.04081922397017479, -0.3829091191291809, 0.11541183292865753, -0.02844519354403019, -0.02779555320739746, -0.07482347637414932, 0.19994238018989563, 0.5092353224754333, 0.28728920221328735, -0.23965321481227875, 0.12303855270147324, 0.2050652652978897, -0.0664573684334755, -0.09601019322872162, -0.21021486818790436, 0.04158385097980499, 0.118598073720932, -0.28382062911987305, -0.21279090642929077, 0.2378080189228058, 0.27622923254966736, 0.3271915912628174, 0.1671748161315918, 0.1932981312274933, -0.44650253653526306, 0.20307332277297974, -0.2077329009771347, -0.08832715451717377, 0.26598602533340454, -0.2979905307292938, 0.29016485810279846, -0.12424731254577637, -0.5077656507492065, -0.5908728837966919, -0.09025224298238754, -0.16900229454040527, 0.16843131184577942, 0.3161853551864624, 0.12544220685958862, -0.13229885697364807, 0.007234059274196625, 0.34984907507896423, -0.8305873274803162, 0.09299828112125397, 0.3387039303779602, 0.5715384483337402, -0.16232062876224518, -0.4159929156303406, 0.137867271900177, 0.13089951872825623, 0.2897316813468933, 0.05746496096253395, -0.441337525844574, -0.12691739201545715, 0.02010858803987503, -0.31128185987472534, 0.5992779731750488, 0.5723857283592224, -0.07950277626514435, -0.19637629389762878, -0.006056971848011017, 0.25576430559158325, -0.15153110027313232, 0.1104859933257103, -0.4583752155303955, 0.4540807008743286, -0.12078867852687836, 0.17887932062149048, -0.09389261901378632, -0.060873664915561676, 0.5435922145843506, -0.08490338176488876, -0.24326299130916595, -0.2590586841106415, 0.3706624209880829, -0.21809709072113037, 0.6111344695091248, -0.5113142132759094, 0.16185949742794037, -0.3057080805301666, -0.4250176250934601, -0.13413356244564056, 0.026609279215335846, 0.12538117170333862, 0.06944853812456131, -0.192273810505867, -0.06775481253862381, -0.37020131945610046, -0.13439027965068817, -0.18708623945713043, -0.2758626937866211, 0.47515222430229187, -0.15910901129245758, 0.5573099851608276, 0.18666447699069977, -0.010905683040618896, -0.26392292976379395, 0.17404469847679138, -0.16635015606880188, -0.6459066271781921, 0.32593220472335815, 0.22413085401058197, -0.48388609290122986, 0.15438120067119598, -0.4510522484779358, -0.18329885601997375, -0.29747915267944336, 0.19039146602153778, -0.10952946543693542, 0.47377315163612366, -0.2330225706100464, 0.03256161883473396, -0.4484539031982422, -0.3853423595428467, 0.44904738664627075, 0.11913329362869263, -0.418873131275177, -0.2332298755645752, 0.23853348195552826, -0.45469942688941956, 0.1581902801990509, 0.0798821747303009, -0.12269875407218933, -0.17575319111347198, 0.07545161247253418, 0.4007767140865326, -0.009623456746339798, 0.2266223132610321, -0.3531486988067627, 0.29148632287979126, 0.0009646061807870865, 0.14686071872711182, 0.37267524003982544, -0.5195629596710205, -0.24383185803890228, 0.6559601426124573, -0.5430050492286682, -0.15136627852916718, -0.5590025782585144, 0.5414517521858215, 0.15054643154144287, -0.09316480159759521, -0.4898378551006317, -0.03905950486660004, 0.1165243536233902, 0.37580692768096924, -0.34512466192245483, -0.1728000044822693, -0.04662699997425079, 0.1917625516653061, 0.1503867208957672, -0.023621149361133575, 0.17137667536735535, 0.17633268237113953, -0.18115991353988647, -0.048317909240722656, -0.5418635606765747, -0.003847278654575348, -0.06959596276283264, 0.1839040070772171, -0.2589517831802368, 0.41360369324684143, 0.46397507190704346, -0.43416792154312134, 0.290994256734848, -0.28121933341026306, -0.2598593831062317, -0.3733299970626831, -0.24058258533477783, -0.37938424944877625, -0.002956502139568329, -0.1525319218635559, 0.15571892261505127, -0.07761713862419128, -0.018582889810204506, 0.18126609921455383, 0.12005358934402466, 0.30920830368995667, 0.5661349296569824, -0.5407423973083496, 0.11887619644403458, 0.20167678594589233, -0.7012468576431274, -0.4558364748954773, -0.28299784660339355, 0.13768711686134338, -0.22309812903404236, 0.27306652069091797, 0.20377397537231445, -0.27487027645111084, -0.0181344635784626, -0.020824730396270752, -0.20679304003715515, -0.2394925206899643, 0.030652320012450218, -0.1987142264842987, -0.01796608418226242, 0.5124882459640503, -0.6328257322311401, 0.5482826828956604, 0.2696896493434906, 0.06993339210748672, -0.21518485248088837, 0.4353078007698059, -0.0831107646226883, 0.06019918620586395, -0.08966715633869171, 0.05868230015039444, -0.204131618142128, -0.12257102131843567, -0.11005230247974396, -0.15033814311027527, 0.16241823136806488, -0.35810768604278564, 0.18074080348014832, -0.24505437910556793, 0.10531575977802277, 0.3686649203300476, 0.37590715289115906, -0.01382242888212204, -0.16342592239379883, -0.055442169308662415, 0.6645841598510742, -0.005517333745956421, -0.039298612624406815, 0.15906833112239838, 0.35680240392684937, -0.52316814661026, -0.11923743039369583, 0.453048437833786, -0.17487837374210358, 0.21983325481414795, 0.22477850317955017, 0.17668268084526062, 0.08219853043556213, 0.09249156713485718, -0.25534260272979736, 0.23476235568523407, -0.09892807900905609, -0.4913265109062195, 0.641808271408081, -0.0833122730255127, -0.18510302901268005, -0.20592395961284637, 0.2695523500442505, 0.25501447916030884, 0.04676799476146698, -0.58066725730896, 0.10834595561027527, -0.3116697072982788, -0.12123674154281616, 0.16393685340881348, 0.1573013961315155, 0.1289089024066925, 0.22195538878440857, -0.36557334661483765, 0.665079653263092, -0.22760596871376038, 0.061643823981285095, 0.5126720070838928, -0.3383682370185852, -0.5719160437583923, -0.20695289969444275, 0.4622761309146881, 0.13975971937179565, 0.03840303421020508, 0.11189945787191391, 0.43382173776626587, 0.215281143784523, -0.224320188164711, 0.08053094148635864, 0.056862883269786835, -0.19457849860191345, -0.3879495859146118, -0.5216328501701355, -0.6656341552734375, -0.7061948776245117, -0.11807416379451752, 0.15289419889450073, -0.4767691493034363, -0.08159711956977844, -0.11001113057136536, -0.26599180698394775, -0.07755538821220398, 0.7177126407623291, -0.26060783863067627, -0.11236155033111572, 0.2309666872024536, 0.3710537552833557, -0.34406906366348267, 0.08131097257137299, 0.5273950695991516, 0.2511974275112152, -0.00897417962551117, -0.5153931379318237, -0.34314510226249695, -0.0029469430446624756, -0.3874833285808563, 0.04276536405086517, 0.32272106409072876, 0.08416736125946045, -0.1406516134738922, 0.03113803267478943, 0.41389474272727966, -0.06736505031585693, -0.5939752459526062, -0.23016393184661865, -0.024745650589466095, 0.577786922454834, -0.3688203990459442, -0.22229409217834473, 0.5306106805801392, 0.24614103138446808, 0.11979743093252182, 0.4881322979927063, 0.35733741521835327, -0.19853541254997253, 0.3661949634552002, -0.1450461745262146, 0.3543017506599426, -0.8821092247962952, 0.16577871143817902, -0.24734929203987122, 0.302788108587265, 0.11961889266967773, 0.15808042883872986, 0.7218691110610962, 0.2749972641468048, -0.29394036531448364, 0.0830688327550888, -0.07503008842468262, 0.29604029655456543, 0.5577934980392456, -0.6146045327186584, -0.2693153917789459, -0.10723009705543518, -0.20358295738697052, 0.3139830231666565, -0.51033616065979, 0.3059326708316803, 0.6899287104606628, 0.3266565203666687, -0.14414595067501068, 0.030802354216575623, -0.1734202355146408, 0.2756521701812744, 0.3544950485229492, 0.27200502157211304, 0.1887693703174591, -0.0772632360458374, -0.0538126602768898]\n",
      "embedding for New York = [-0.06661737710237503, -0.12932531535625458, 0.3939805030822754, 0.3369292616844177, 0.055773038417100906, 0.296535849571228, 0.3786011338233948, -0.14548179507255554, 0.07794789224863052, -0.05712704360485077, -0.2736111879348755, 0.08959890156984329, 0.10346145182847977, -0.42712345719337463, -0.2510644793510437, 0.1164967268705368, -0.006012454628944397, -0.3515355587005615, -0.19887582957744598, -0.5188259482383728, -0.28594771027565, -0.4518674314022064, -0.14513711631298065, 0.08973868191242218, 0.04313702508807182, 0.38956838846206665, -0.1843183934688568, 0.3283308744430542, -0.14797747135162354, 0.09110333770513535, 0.08598525077104568, -0.13584032654762268, 0.2869311571121216, 0.3877931237220764, -0.33332759141921997, 0.16659343242645264, -0.27472496032714844, -0.003738122060894966, 0.17138174176216125, 0.2551482021808624, -0.23659729957580566, 0.5148249268531799, -0.28177449107170105, 0.022580131888389587, -0.3602186143398285, 0.4589518904685974, -0.313836008310318, 0.18571601808071136, -0.13402460515499115, -0.13676364719867706, -0.01564907655119896, -0.531326413154602, 0.6050004363059998, -0.24953718483448029, -0.011783398687839508, 0.23130479454994202, -0.31073445081710815, 0.5233226418495178, -0.27350521087646484, -0.276478111743927, 0.5981225967407227, 0.3368629813194275, 0.18691937625408173, -0.4098828434944153, -0.2878052592277527, 0.3864499628543854, -0.23537445068359375, 0.09117035567760468, 0.12791255116462708, -0.15236161649227142, -0.109153613448143, -0.16201232373714447, 0.22652292251586914, -0.12626521289348602, -0.37651747465133667, -0.22804471850395203, 0.18710850179195404, -0.16626495122909546, 0.43233031034469604, -0.1500391960144043, -0.22633817791938782, -0.23951032757759094, 0.0461713932454586, 0.17925205826759338, 0.017335141077637672, 0.4944688677787781, -0.14177662134170532, -0.05003877729177475, 0.151799276471138, -0.13893412053585052, 0.5930785536766052, 0.3303920328617096, -0.4489279091358185, -0.3110787868499756, 0.05907239764928818, -0.3472047448158264, 0.06094777211546898, -0.22932370007038116, -0.36894750595092773, -0.1753285825252533, -0.3647805154323578, -0.14388824999332428, 0.3887636065483093, 0.048386722803115845, -0.15286622941493988, 0.12971004843711853, -0.12710753083229065, -0.032273925840854645, 0.30586475133895874, -0.2230415642261505, -0.15794219076633453, -0.17503142356872559, -0.5677199363708496, 0.07313767075538635, -0.18086764216423035, 0.04960113763809204, 0.12293680012226105, -0.15031380951404572, -0.3168351948261261, -0.15514296293258667, -0.009288415312767029, 0.3961983025074005, 0.10286818444728851, 0.4243260324001312, -0.06583788990974426, -0.08894245326519012, -0.06235286965966225, 0.23489142954349518, -0.1738530993461609, -0.6761564016342163, 0.4671463668346405, 0.03149901330471039, 0.5298769474029541, -0.1608426719903946, -0.0398571714758873, -0.1761917918920517, -0.196396604180336, -0.20928740501403809, 0.02089916542172432, -0.219445139169693, -0.04759593680500984, 0.6210812926292419, 0.13425979018211365, -0.11833994090557098, 0.21662290394306183, -0.4560561180114746, -0.23968324065208435, -0.08342543244361877, -0.13366666436195374, -0.2287990152835846, 0.18719635903835297, -0.12503111362457275, -0.18897783756256104, 0.18535609543323517, 0.40116697549819946, -0.0454772412776947, 0.17097848653793335, -0.24245744943618774, 0.34754636883735657, 0.4048755466938019, -0.2701607346534729, 0.3492735028266907, 0.34354540705680847, -0.3535333275794983, 0.16565628349781036, 0.08945050090551376, 0.438814252614975, 0.02611413598060608, -0.1272178292274475, 0.17809805274009705, 0.22623996436595917, 0.25632742047309875, -0.3076390326023102, 0.13547387719154358, 0.3036584258079529, -0.16058363020420074, 0.0798293799161911, -0.25113141536712646, 0.11131465435028076, -0.12878158688545227, 0.4615197777748108, 0.4893988072872162, -0.48813849687576294, 0.23243390023708344, 0.20999157428741455, -0.3517805337905884, 0.10010853409767151, -0.4362259805202484, -0.09693793952465057, 0.355512797832489, -0.27976754307746887, -0.42642587423324585, 0.011199228465557098, -0.28710317611694336, 0.0524042546749115, -0.008291289210319519, -0.3150366246700287, 0.3605991303920746, -0.22197434306144714, -0.4181698262691498, 0.14297056198120117, 0.2681037187576294, -0.27843189239501953, 0.029922371730208397, -0.11235378682613373, -0.38610702753067017, -0.007013579830527306, 0.32947224378585815, -0.6084492802619934, 0.1802036464214325, -0.014395900070667267, 0.03666485846042633, 0.2937568426132202, -0.004988670349121094, -0.14558236300945282, -0.11315352469682693, -0.11775833368301392, -0.21369825303554535, 0.10305039584636688, -0.2709703743457794, -0.2449015974998474, -0.1599014401435852, 0.14799180626869202, -0.4656037390232086, -0.38868412375450134, 0.025611557066440582, -0.09277620911598206, 0.28436869382858276, 0.222552090883255, -0.1343858242034912, -0.18534758687019348, -0.15997916460037231, -0.10884109139442444, 0.011127885431051254, -0.009445911273360252, 0.5957270860671997, -0.07895398139953613, 0.1557084172964096, 0.026899471879005432, -0.05682195723056793, -0.1808740794658661, -0.08941062539815903, 0.09912930428981781, -0.5088853240013123, -0.28116053342819214, 0.1964322030544281, 0.1579149216413498, 0.19258716702461243, -0.2852939963340759, -0.15465927124023438, 0.034279584884643555, 0.3591557741165161, -0.12691186368465424, -0.27394163608551025, -0.09910900145769119, -0.04421931505203247, -0.14302729070186615, -0.2312781810760498, -0.4183698892593384, 0.2469945251941681, 0.11357243359088898, 0.293889582157135, 0.07280762493610382, 0.21297770738601685, 0.40047019720077515, 0.08988244086503983, -0.08387807756662369, -0.07101806998252869, -0.2035108506679535, -0.2703375816345215, -0.0891151875257492, -0.10635695606470108, 0.41107770800590515, -0.1595212072134018, 0.30267447233200073, -0.07423225045204163, 0.16552571952342987, -0.40940582752227783, -0.7010321617126465, -0.5206790566444397, -0.06410881876945496, 0.2608192265033722, 0.12399362027645111, 0.2941057085990906, -0.020268514752388, -0.24675485491752625, 0.20413047075271606, -0.470575749874115, 0.13052640855312347, -0.02584005892276764, 0.2271212935447693, -0.2927418351173401, 0.13405990600585938, -0.11891473829746246, -0.7462047934532166, 0.23010055720806122, 0.008117949590086937, -0.006207004189491272, 0.16209401190280914, 0.030484579503536224, -0.12293781340122223, -0.44477078318595886, 0.05656643211841583, -0.22128142416477203, -0.02180783450603485, 0.3619449734687805, 0.03500422462821007, -0.457406610250473, -0.3696749210357666, 0.005321428179740906, -0.1995573341846466, -0.3399837911128998, 0.16701504588127136, -0.4245590567588806, 0.3691808879375458, 0.39359188079833984, -0.054903630167245865, 0.10750184208154678, -0.12514403462409973, -0.5748754739761353, 0.008128777146339417, 0.4149935245513916, 0.05561655014753342, -0.1181102991104126, 0.20850837230682373, 0.0202818363904953, 0.24009951949119568, -0.7010267376899719, -0.21736064553260803, 0.06782492995262146, 0.10288183391094208, -0.5090674757957458, 0.14293314516544342, 0.023211143910884857, -0.21526841819286346, -0.02193666808307171, -0.18249903619289398, -0.07222114503383636, 0.760391891002655, 0.1461094617843628, -0.41934123635292053, 0.21969452500343323, -0.27365344762802124, -0.6230558753013611, 0.2821125090122223, 0.6126574277877808, 0.11126092076301575, -0.08786617964506149, 0.6018292307853699, 0.2422163486480713, -0.1866171509027481, -0.09224745631217957, -0.020783916115760803, -0.15777352452278137, 0.4202592372894287, -0.4585856795310974, -0.5546536445617676, -0.15593251585960388, 0.17645151913166046, -0.22232595086097717, -0.2090613692998886, -0.4970109462738037, 0.04024270921945572, 0.03764818608760834, 0.2981735169887543, -0.2107604742050171, -0.32452237606048584, -0.2291242480278015, 0.10149215161800385, -0.022730078548192978, -0.04943995177745819, 0.041688427329063416, 0.5319369435310364, -0.2906300127506256, 0.2725038528442383, -0.29932069778442383, 0.6787280440330505, 0.2734854519367218, -0.10544933378696442, 0.6171223521232605, 0.43362003564834595, -0.22788532078266144, -0.4443325996398926, -0.07602842152118683, 0.023368552327156067, -0.22344641387462616, -0.05957035720348358, -0.04105246067047119, -0.2539528012275696, -0.1668982207775116, 0.38735994696617126, 0.30961087346076965, -0.26767924427986145, -0.33673006296157837, 0.005898535251617432, -0.42626309394836426, 0.10847640037536621, 0.034417927265167236, 0.19595617055892944, -0.18099123239517212, -0.06694423407316208, 0.1930493265390396, -0.1304894983768463, 0.11386536806821823, 0.06817372143268585, 0.22343775629997253, 0.1040726900100708, 0.2701053023338318, -0.23039311170578003, 0.09200745820999146, -0.03589056804776192, 0.30055856704711914, 0.055574603378772736, 0.16905924677848816, 0.03240996599197388, -0.10513058304786682, 0.14721259474754333, 0.2261209934949875, 0.32959115505218506, 0.1660332828760147, 0.1515759974718094, -0.43353933095932007, 0.4141111373901367, 0.47089722752571106, -0.1939958930015564, 0.5524909496307373, -0.3170406222343445, 0.3685077726840973, 0.019991088658571243, -0.5100399851799011, -0.0010614487109705806, 0.02861201763153076, 0.12256882339715958, -0.6132559776306152, 0.04957102984189987, -0.4452645480632782, 0.18021473288536072, -0.048591263592243195, -0.2638547718524933, -0.3359263837337494, 0.01028474047780037, 0.1543523073196411, 0.06281104683876038, 0.19980479776859283, -0.04356909170746803, -0.3542154133319855, -0.5657062530517578, -0.0906941145658493, 0.10746940970420837, 0.02122747153043747, 0.22128871083259583, 0.39649078249931335, 0.04412884637713432, 0.09539888054132462, 0.26407700777053833, -0.19001862406730652, -0.09114043414592743, 0.026054367423057556, -0.19933727383613586, 0.11110937595367432, 0.13263511657714844, 0.48591089248657227, -0.14210087060928345, 0.34165671467781067, 0.07421471178531647, -0.16859379410743713, 0.04283493384718895, 0.10893696546554565, 0.09762705117464066, 0.19688770174980164, -0.12194408476352692, 0.2971871495246887, -0.14270168542861938, 0.3514394164085388, -0.0028060432523489, 0.31183844804763794, -0.18369098007678986, -0.2211771309375763, -0.5827292203903198, 0.14812667667865753, 0.759447455406189, -0.5448870658874512, 0.15066170692443848, -0.15287309885025024, -0.023951761424541473, 0.12038791179656982, -0.2715531587600708, 0.642700731754303, 0.3024534583091736, -0.016342900693416595, 0.18454121053218842, 0.38014671206474304, -0.4282118082046509, -0.4190327227115631, -0.26990389823913574, 0.19884330034255981, 0.26861572265625, 0.27660274505615234, 0.10686300694942474, -0.15243485569953918, -0.019819580018520355, 0.13201157748699188, 0.13051292300224304, -0.17279557883739471, 0.13061873614788055, 0.3532998561859131, -0.18227708339691162, 0.13218630850315094, 0.3197500705718994, 0.48577073216438293, -0.30299144983291626, 0.029064692556858063]\n"
     ]
    }
   ],
   "source": [
    "for w in ['canada', 'ghjffg', 'cyclotron', 'New York']:\n",
    "    print('embedding for {0} = {1}'.format(w, elmo.emb(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elmo encode any combination of characters even it is not exist in English dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
